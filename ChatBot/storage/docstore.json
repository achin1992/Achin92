{"docstore/metadata": {"4632de4d-db37-491c-b70d-3e7740dfdae5": {"doc_hash": "a2a805cb9a11aeb1419914d2bfd61b923500d3d4f8c672cf5b8b4b0c255857fe"}, "87e59040-0e3d-4d29-ac73-6666cb8c4440": {"doc_hash": "c5e5ed57d8a12462a0dd55a5cb800e18b374776af07897dd7932880e37871d15"}, "652fe5bc-5b76-46c9-a3af-d2cd5d0b5b8e": {"doc_hash": "20262f7faaf6c0b6ca8391ffbed7201c65e06790a66c4d0b6c1dfa5e10fed64b"}, "0c4b39a6-7503-4ca9-9446-3a83ae2eed34": {"doc_hash": "6e3e5337238a1f4ac1c1bb1adc393fa52dd455cc2abf0aaa701833a203d8b6d3"}, "5eed7f71-92c7-47b1-ac9d-50e9548dd537": {"doc_hash": "3f0c381451d35b34ce7b6ae308eaddcbd1402e402a5ae50687befc39f8b6ff95"}, "bcc7d2ad-e32c-4171-874c-bf2d5fe69c5f": {"doc_hash": "422f419325520a270b0d1368314347321acfc484fb72dca9f7b43957f49d6945"}, "768622ef-31a3-49dd-8de4-0043010c7bc9": {"doc_hash": "0bca31e1ec33090d68a34523e2f6d3f39b0038769c69fc22486353d9f1272670"}, "691a64d4-db7c-4b2f-911c-e6e6223326d0": {"doc_hash": "27dc77131dede4c35816f9f9dd7f8982d2a2ffc4f7aca81b0f134a9d0a8929c1"}, "eafad47f-6db6-44d9-9513-3fbdabb3fd5c": {"doc_hash": "b2d55cc83ab9dfe2d0efbb7dcbd6a3427cd76b784322d77bb0decb80e206ee8b"}, "afa2c7ce-d93c-4acb-a3f2-9063ef2a043b": {"doc_hash": "787cfec2be6611c5bbadaea45680f69f1a015646c29ae3e2cc81cdc592b4892e"}, "61d5c535-efe6-4331-b2d2-52dda0ca0717": {"doc_hash": "3acc16c816c1248678b3ac24e945db2b8901b31d63f927cb3cef64afceb56a0b"}, "e346ceeb-1c15-42ad-84ba-fd5522608f29": {"doc_hash": "561b0c33e8c0cf3cf7a4315212dc84d570a4f2ad14ddcf8f63ad7ff5a437969e"}, "116c8dbf-037a-4200-be81-e5c0319e9698": {"doc_hash": "ea477bf7924e6e10a8f2de8bf85fb7caf7780d6a472f3b33617e3d84aac83cfd"}, "db5fb2f4-d2f0-4d7a-bd62-c66248922ff3": {"doc_hash": "c0672c2a6e46d4922604ff78d5c9fd985afd99b950735cbdadef47eba7a1f200"}, "f1f6da0b-1deb-4625-9d20-4f6720269576": {"doc_hash": "43a76a81814dda390ea006a701e9f5be58b28c8368c3fc94bafdbee98b473fa3"}, "0ac82c0f-cd7c-47e0-9ff0-aa98f437a149": {"doc_hash": "36fe76ac2446bba50e3239f09c810ceb8dde8129362fbd971c57210adcb9b24b"}, "8e6f8f16-8a3a-4bf5-8b04-0527eb29786c": {"doc_hash": "2e020784239db152094a4530bb418d243d243fcddee796a6dc89521fd5100a4c"}, "753a4375-d945-496b-b0f9-751cb0c44b5a": {"doc_hash": "00f60defe700578bd653c9acb4ca23d4f6ca834fa06e1454877ac12b307ded08"}, "1dc95323-f764-49e5-8bc9-7e1f7a8c526c": {"doc_hash": "a846ec3903104e40f6563f33979de5ec9ffc8e0a27cc9a28fd59a501f144f417"}, "b47d24b0-5040-4c7c-8b6e-2beeb5d904d7": {"doc_hash": "7ef91d5b10986a4631ce434a974da186346ffd9fe47faa3f19b24c6eb82ec99d"}, "a056aca2-ffcc-4d8b-a416-ddfe4e048b1e": {"doc_hash": "167dfbc12ed901bf9d3cd167b34414462cdbf3b123e57c3d229a9c3e2cda7be9"}, "e6923e67-679c-4881-af4d-3a50ba1f1fd1": {"doc_hash": "9d5b682c62f555912c596c214c47eab6ba7d5b28e64afe3a7979f05c01a679a6"}, "0f3bb16b-320f-47b1-be61-26b98e52f6e3": {"doc_hash": "4fe14189dd63123a94c0f8981b1ae22323444ae5b2e237ffe310077bccd9cf23"}, "38477e1a-89a1-4db3-8d3b-140fd93614f7": {"doc_hash": "7469f1917949f05a6e6abb2f69f975ade2200d8a55f44854bfed2905e2f94261"}, "44cf608d-4bdd-4f4f-b0a7-bc766fa148bd": {"doc_hash": "3985f12b776f9610c818856a16414b741614bda6fb0d8704571adc88ad776c8d"}, "a3d150e6-0daf-4eb8-8ad2-0fe8e9f9e55c": {"doc_hash": "155ee1bdaf26d97c11d9ee7d0db956245aeabc0029893fafbba567dbbd5e9085"}, "9aa51383-b563-46bd-b62d-cef1f558353c": {"doc_hash": "ac922782f5e79ac1646155faad2210789af4ecd70fc6e0d01b5a7cd7af4537bd"}, "6d86bbde-2abd-45dd-9a14-8f4f34f430e6": {"doc_hash": "82e5375390981597062078e909c5e3036a520e7ab5c3054a446fb10d5b464fe2"}, "0ee29ad9-bc57-4b31-a6dc-9e87108f72f0": {"doc_hash": "64aabf3d8bf24a2b57691e476baabe9f8695b25cf1af6593a64687d46d39d4de"}, "db1976cc-e5b7-471a-9c7f-2a38bdae3a14": {"doc_hash": "de6f69c78e220346a65a49eedaf0dd8bb8260e6ec2975bf819c7186e5909e8e7"}, "51abef16-717a-4c3a-b3fd-3c1244771028": {"doc_hash": "3d68a28440ac921d284180506851dda1820338ee2651aa4c46048edda68c0a5f"}, "80fb44e1-1a80-4d00-bdfd-c1e4ecc5e4d2": {"doc_hash": "53d23420100bc2e192be2bca9948b6a8e6f50caf37d7f29609eda85c94d0a2e6"}, "c154b844-b168-49a6-81b7-e52547495d8c": {"doc_hash": "d08a0acf5f40ddcb1768494387b7a89733822d279cd11ae40d06b85f7edefa1b"}, "6351959d-3b85-443e-a376-5419af675ef4": {"doc_hash": "50df1ea11844ace82af8d44702ec25a29c2ee001f7fc0a0a52a0cc48b416da68"}, "dafbbfc1-48c8-4e4b-98b6-51552acfd11a": {"doc_hash": "7bfcb665972e5dacd1b23f65b680af2882063b3c359e4f1bffbe5d0059705f6c"}, "d842ad18-e5d0-4f2d-ab42-96e701ad2373": {"doc_hash": "1bed2532a71dcc8d9d8094756eb8b8d5d5ee8905a18ac005c03e1ff773117018"}, "b66d45d2-9202-450c-b651-c9c52cf80a1d": {"doc_hash": "20910a2c040053c7c6a7f06513058d3ae7b28730fd7080f2fd4ae26d20967a4b"}, "97ac72b1-4dfa-47b4-8912-90b5d2972d8b": {"doc_hash": "d9b4ef8f77ef81133ddf69fe8c7482cad5359f4f8ab0f0f627d172897e3b0fb0"}, "e9cbbd90-0a78-4555-b8c7-ef29177cca7e": {"doc_hash": "9f6929d1d9ae85ee632076cb1ffc6e346ac50221f57bff585266831d26f7c96b"}, "00c63747-3875-45ca-9919-555bc1869ff8": {"doc_hash": "2d81865b6663c497c47ef6856d20359bc2a786e3203c66a4afa2b234b6660541"}, "abfeea92-acd0-41a9-b116-7b5f5eb2899c": {"doc_hash": "261d3d9f58cc9e7253e83d377f974bd25fa140e1503bead67708ecb5bbc00cda"}, "2b07d91f-1c85-4289-a7ba-30cddd3958a1": {"doc_hash": "9acb8c891ba665e870a66e89e03802add3e779184a7a134b540bb7a28d19ad1c"}, "f0c8453c-e8fe-44e9-a622-2a028b642c30": {"doc_hash": "f9d11148b9762f86891df5bded85bc3ddb7038a0439576d7928398cb19586e6f"}, "9be49005-2a1c-4530-8a69-f1ea20aa3c71": {"doc_hash": "8c0e8aef2cde7b053b9a98058f24f1ae169abc15969e929bcfe92f58f184ff1f"}, "9b403986-af9b-4a5f-9776-f93b30d8e2da": {"doc_hash": "a54195808434df9b07c8e72df56f645156676866d0d1e1b77478d800957e4c45"}, "0a9a86e8-8305-4493-9040-3b1c41f9726b": {"doc_hash": "2bc46b5e8f96c3b230a6b88fc16c7c429f6e52b1c36897d65ef0e1dac22cd836"}, "95f6aaa7-1b35-4200-b3de-75b233d37679": {"doc_hash": "5ffa1ebed1f7c7d3a6fd6b26b3aee1ba868d945d74977b84e9134baf116126b9"}, "4667514a-1b33-42e0-b181-bdba9a4eadd4": {"doc_hash": "859b0a0164198c8166460e6bb0d3562a14194fc4435de6a0c2782209e5ec4f5e"}, "c6e49391-7565-46ae-9529-647c2b25c634": {"doc_hash": "21b3d2c77af701b519b120791f55991a4d76bc8e734e688ae29c8db79232ebc0"}, "3fe4e6a5-4bc2-4840-859a-4846053136fc": {"doc_hash": "35009c74b55bccbcae9aed4e487bd09eeacdfd087e2a4d4440a8e92a91432cb9"}, "8a380bfe-8a4f-463b-a4ef-1daeed0d6984": {"doc_hash": "692d389e11aeb859899890e6da0c9231ee46b3f1acc15ef4d9e136abb33e937d"}, "68ae190f-7e02-495e-83ea-3b834147d256": {"doc_hash": "c5e5ed57d8a12462a0dd55a5cb800e18b374776af07897dd7932880e37871d15"}, "eb674d8b-2aec-4437-8d81-888b95da3b96": {"doc_hash": "20262f7faaf6c0b6ca8391ffbed7201c65e06790a66c4d0b6c1dfa5e10fed64b"}, "b38c771b-9a24-4fb0-bfd5-bf1ef86b5e97": {"doc_hash": "6e3e5337238a1f4ac1c1bb1adc393fa52dd455cc2abf0aaa701833a203d8b6d3"}, "87d63a19-01f5-4fba-ad33-28a944c52ad4": {"doc_hash": "3f0c381451d35b34ce7b6ae308eaddcbd1402e402a5ae50687befc39f8b6ff95"}, "ab477405-457d-421d-a3ad-15467c5a9762": {"doc_hash": "422f419325520a270b0d1368314347321acfc484fb72dca9f7b43957f49d6945"}, "ba6c56bb-536f-49e4-b57a-ef05088e22bc": {"doc_hash": "0bca31e1ec33090d68a34523e2f6d3f39b0038769c69fc22486353d9f1272670"}, "32368d5d-c962-4fbc-99c5-f5b923d1957f": {"doc_hash": "27dc77131dede4c35816f9f9dd7f8982d2a2ffc4f7aca81b0f134a9d0a8929c1"}, "508721b8-cb18-4115-87ac-28d5b284d4ba": {"doc_hash": "b2d55cc83ab9dfe2d0efbb7dcbd6a3427cd76b784322d77bb0decb80e206ee8b"}, "dd23d8bf-ed1a-4fcc-8322-5e27d60843ca": {"doc_hash": "787cfec2be6611c5bbadaea45680f69f1a015646c29ae3e2cc81cdc592b4892e"}, "045d87d4-2d35-4794-8fcf-f64219687636": {"doc_hash": "3acc16c816c1248678b3ac24e945db2b8901b31d63f927cb3cef64afceb56a0b"}, "ebfd07ff-6b10-4068-8245-9567603012da": {"doc_hash": "561b0c33e8c0cf3cf7a4315212dc84d570a4f2ad14ddcf8f63ad7ff5a437969e"}, "e235e797-b7ff-4299-8e59-8dc2ebeb4921": {"doc_hash": "ea477bf7924e6e10a8f2de8bf85fb7caf7780d6a472f3b33617e3d84aac83cfd"}, "e2f9288d-599c-4971-9602-b98426ce1ded": {"doc_hash": "c0672c2a6e46d4922604ff78d5c9fd985afd99b950735cbdadef47eba7a1f200"}, "661f0b51-3fbc-4dda-b4ff-faaf41c4c843": {"doc_hash": "43a76a81814dda390ea006a701e9f5be58b28c8368c3fc94bafdbee98b473fa3"}, "2dec25b1-d406-410a-991f-acc3a173e05d": {"doc_hash": "36fe76ac2446bba50e3239f09c810ceb8dde8129362fbd971c57210adcb9b24b"}, "d9e9451f-5a46-4831-91a4-80fef3cc9870": {"doc_hash": "2e020784239db152094a4530bb418d243d243fcddee796a6dc89521fd5100a4c"}, "33713b07-43ac-4eda-813a-dfc0f1895573": {"doc_hash": "00f60defe700578bd653c9acb4ca23d4f6ca834fa06e1454877ac12b307ded08"}, "5e1bcafc-a3e4-4f71-b71d-60023a8eb4e9": {"doc_hash": "a846ec3903104e40f6563f33979de5ec9ffc8e0a27cc9a28fd59a501f144f417"}, "b038d056-7606-4f71-b397-599027cd66a4": {"doc_hash": "7ef91d5b10986a4631ce434a974da186346ffd9fe47faa3f19b24c6eb82ec99d"}, "71ad4c76-4d00-47bc-8117-97ecd00de777": {"doc_hash": "167dfbc12ed901bf9d3cd167b34414462cdbf3b123e57c3d229a9c3e2cda7be9"}, "b5017d41-ed6a-471d-a332-a30527a59782": {"doc_hash": "9d5b682c62f555912c596c214c47eab6ba7d5b28e64afe3a7979f05c01a679a6"}, "5ad07fdc-a14b-47ef-b211-e2e12d39411c": {"doc_hash": "4fe14189dd63123a94c0f8981b1ae22323444ae5b2e237ffe310077bccd9cf23"}, "4ec7c74c-bb46-41b4-a705-4e2d5783ec7b": {"doc_hash": "7469f1917949f05a6e6abb2f69f975ade2200d8a55f44854bfed2905e2f94261"}, "a96dc815-3bd6-4b57-8981-f62394a4f93e": {"doc_hash": "3985f12b776f9610c818856a16414b741614bda6fb0d8704571adc88ad776c8d"}, "5d615a6d-f425-493f-9b82-1815ff0a223b": {"doc_hash": "155ee1bdaf26d97c11d9ee7d0db956245aeabc0029893fafbba567dbbd5e9085"}, "1e41b9a9-3f49-4fde-8997-08dd4dc22756": {"doc_hash": "ac922782f5e79ac1646155faad2210789af4ecd70fc6e0d01b5a7cd7af4537bd"}, "4b0d2c59-205e-4c05-a65d-df5dcf4eee62": {"doc_hash": "82e5375390981597062078e909c5e3036a520e7ab5c3054a446fb10d5b464fe2"}, "8bac434a-3bef-4f11-aef2-2b646984b09f": {"doc_hash": "64aabf3d8bf24a2b57691e476baabe9f8695b25cf1af6593a64687d46d39d4de"}, "10c833d9-eda6-4c78-82c8-1153167f9291": {"doc_hash": "de6f69c78e220346a65a49eedaf0dd8bb8260e6ec2975bf819c7186e5909e8e7"}, "8842e3dd-3d9e-436f-ba6f-46cd009d2030": {"doc_hash": "3d68a28440ac921d284180506851dda1820338ee2651aa4c46048edda68c0a5f"}, "6ccd872f-ceef-43e5-a9b5-38570afb9b43": {"doc_hash": "53d23420100bc2e192be2bca9948b6a8e6f50caf37d7f29609eda85c94d0a2e6"}}, "docstore/data": {"c154b844-b168-49a6-81b7-e52547495d8c": {"__data__": {"text": "problem\n\ncontent users were experiencing slowness in snl document form and snl docviewer in westx. \n\nsolution\n\nmultiple support teams (dse, documents and filling b, vmo, storage network support) were engaged to investigate the issue. the dse team identified that there was a blockage on the bizwire loader (bizwireload.exe) on the snldb2 server. documents filling b team stopped bizwire services on snl process (1, 7 16) and then re-enabled them. documents filling b team has removed the old 30k and 0kb documents from the input folder and also reduced the threading from 15 to 10 per server to mitigate the issue \n\nproblem\n\ncontent users were experiencing slowness in snl document form and snl docviewer in westx. \n\nsolution\n\nit was reported that multiple content applications (cts, west, westx, extractor, docviewer, celsus, pathfinder) across multiple locations (amd, hyd, manila and ggn) were impacted. network data team confirmed that there was a network interruption with zayo 10g circuit between cho and ash data centre which resulted in routing hang state, which led to network interruption for the traffic passing between both data centers. this is a known risk (r127) which is already logged. network data team removed and re-added the static route to mitigate the issue\n\n\n\nproblem\n\nmultiple content applications (cts, west, westx, extractor, docviewer, celsus, pathfinder) were impacted. gcc application was also impacted.\n\nsolution\n\nit was reported that multiple content applications (cts, west, westx, extractor, docviewer, celsus, pathfinder) across multiple locations (amd, hyd, manila and ggn) were impacted. network data team confirmed that there was a network interruption with zayo 10g circuit between cho and ash data centre which resulted in routing hang state, which led to network interruption for the traffic passing between both data centers. this is a known risk (r127) which is already logged. network data team removed and re-added the static route to mitigate the issue\n\n\n\nproblem\n\nusers are experiencing issues with zscaler zpa.\n\nsolution\n\niam ops team performed a configuration change to modify zpa birthright access rule for ratings users in north america (ritm1213184). this config changes disabled end-users sailpoint access, causing the issue. the access rule change was reverted by the iam ops team. all the impacted users are added back to the required ad group and the ad group is performing a force sync of profiles all the impacted users are advised to use cisco vpn through their email id and authenticator app as a workaround. zscaler team has confirmed that after the sync of azure ad to zpa scim, 4832 ids are now available and will not have any issues and the issue is mitigated. however, there are 18 missing ids from all divisions, which is a negligible count and can be ignored for now. the ratings im team was able to perform the initial validations and confirmed that 8 ratings users were able to access zpa without any issues. complete validations will be performed on a reconvene call has been scheduled to discuss further course of action. \n\n\n\nproblem\n\npriority score and sort order was not getting assigned in ct admin application due to failure of a job attributed to a wrong value in the crawler admin page. \n\nsolution\n\nit was identified by data collection g team that the job (job_assign document priority rank_prc) on fdca server is failing at 4th step and is throwing the error: arithmetic overflow error for data type tinyint. it was also identified bad element values collected against type id 510 for couple documents (1930490316, 1930490450) in the tables (document element integer_tbl/ document to object real element integer_tbl). the same are deleted by data collection j team from couple tables to resolve the issue. the job is running successfully now.\n\n\n\nproblem\n\npriority score and sort order was not getting assigned in ct admin application due to failure of a job attributed to a wrong value in the crawler admin page. \n\nsolution\n\nthe mt support team observed errors", "doc_id": "c154b844-b168-49a6-81b7-e52547495d8c", "embedding": null, "doc_hash": "d08a0acf5f40ddcb1768494387b7a89733822d279cd11ae40d06b85f7edefa1b", "extra_info": null, "node_info": {"start": 0, "end": 4038, "_node_type": "1"}, "relationships": {"1": "4632de4d-db37-491c-b70d-3e7740dfdae5", "3": "6351959d-3b85-443e-a376-5419af675ef4"}}, "__type__": "1"}, "6351959d-3b85-443e-a376-5419af675ef4": {"__data__": {"text": "in the crawler admin page. \n\nsolution\n\nthe mt support team observed errors on one of the schema servers (choschemaprod01) which seems to be the cause of issue. mt support restarted the schema service to resolve the issue. the content teams confirmed that the content applications are working fine and they are no longer experiencing any slowness.\n\n\n\nproblem\n\nextreme slowness in content collection applications (cts/west/wetx/extractor and docviewer) due to a mt schema server issue \n\nsolution\n\nthe mt support team observed errors on one of the schema servers (choschemaprod01) which seems to be the cause of issue. mt support restarted the schema service to resolve the issue. the content teams confirmed that the content applications are working fine and they are no longer experiencing any slowness.\n\n\n\nproblem\n\ncontent apps dependent on snldb2 and snldb3 were down due to a isp link (fiberlight) flap issue \n\nsolution\n\nmultiple support teams (network data, network security, wintel, dse, content users) were engaged to investigate the issue. the network data team has confirmed that there was an isp link (fiberlight) flap which has caused the issue. the impacted isp link (fiberlight) has been shut down to mitigate the issue and the traffic is traversing through the secondary isp link (zayo\n\nproblem\n\nmultiple users receiving errors when opening stories in ccp \n\nsolution\n\ntechops restarted services on the prod-us-chp1-us-ccp node.\n\nproblem\n\nstale consensus estimates data on ciq and ciq pro due to an informatica issue \n\nsolution\n\nit was identified that intermittent issues were observed between informatica and consensus data refresh service due to which queue built up. although, queue caught up on 4/27, but it was observed that consensus estimates data was reprocessing older messages from informatica and causing bad data. a case with vendor (informatica 04482901) has been logged to identify the cause of the issue and investigate further. ciq dev team identified the jobs which were not processed from the backlogs. im-csd-estimates standardization disabled incremental flow to consensus calculator app, purged the existing queues, post which estimates calculator was brought to process all backlogs. the estimates team completed processing the backlogs for ciq (1.2 million) and ciq pro (3.5 million), to resolve the issue. after restarting informatic ume store by soa team issue got resolved.\n\n\n\nproblem\n\nusers working from manila and pune odc (spgi and ihs) were unable to access internet via lan due to a power failure on a legacy switch in secaucus dc \n\nsolution\n\nthe onsite engineer visited the datacenter and powered on the switch which mitigated the issue.\n\n\n\nproblem\n\nindex metadata feed - error occurred when validating remote object grip.index_dest_for_ids@gripxp \n\nsolution\n\ndv team have executed the arping command for the nutley dns\n\n\n\nproblem\n\ndocuments in edca application were picked up in delay due to timeout error on the sql database server \n\nsolution\n\nmultiple teams (mi database, scrum mount trisul, content sre) teams were engaged on bridge call to investigate the issue. scrum mount trisul team identified that there was timeout error on the sql database server (col21dbsqlprd\\col21), indexing job running slow which had caused the issue. mi database team have killed the indexing job to mitigate the impact validation has been performed by the users which states that auto extraction services are now picking up the documents.\n\n\n\nproblem\n\nexternal clients and ihs users were experiencing intermittent issues with email delays due to an exchange server connection to ad issue \n\nsolution\n\nmessaging team has confirmed that they identified two problematic exchange servers (va06exm5005 va06exm5006) that have caused the intermittent issues and the email delay. messaging team removed impacted servers from the cluster to mitigate the issue. the remaining servers", "doc_id": "6351959d-3b85-443e-a376-5419af675ef4", "embedding": null, "doc_hash": "50df1ea11844ace82af8d44702ec25a29c2ee001f7fc0a0a52a0cc48b416da68", "extra_info": null, "node_info": {"start": 3975, "end": 7875, "_node_type": "1"}, "relationships": {"1": "4632de4d-db37-491c-b70d-3e7740dfdae5", "2": "c154b844-b168-49a6-81b7-e52547495d8c", "3": "dafbbfc1-48c8-4e4b-98b6-51552acfd11a"}}, "__type__": "1"}, "dafbbfc1-48c8-4e4b-98b6-51552acfd11a": {"__data__": {"text": "impacted servers from the cluster to mitigate the issue. the remaining servers (nj12exm5005 nj12exm5006) have processed the backlog messages and the backlogs cleared. no pending emails in the queue.\n\n\n\nproblem\n\ndocuments were not flowing to downstream applications from foreseer application due to few filings mistakenly added to production instead of staging \n\nsolution\n\nextraction a team, identified that the issue was caused due to few filings that were mistakenly added to production by fundamentals content team instead of staging ct admin. due to multiple retry attempts for these bad filings, the queue built up and delay was observed in processing the documents. extraction a team stopped the nifi processes to stop the flow of new filings and cleared the bad filings from queue to resolve the issue. the queue cleared and the nifi process was restarted.\n\n\n\nproblem\n\npublications are not making it to alpha mail. \n\nsolution\n\nmissing reports were manually re-published and dbops cleared the db blocking sessions after which the remaining publications that had been stuck processed automatically.\n\n\n\nproblem\n\ndelay in delivery of profiling data from ciq to ciqpro platform due to changes rolled out in production against a user story \n\nsolution\n\nf1 avengers team identified that the issue was caused due to changes rolled out in production. the data architecture team rolled back the change to fix the issue. however, app pool recycle did not happen at the scheduled time. hence, mi data services b team manually recycled the app pool for the changes to come into effect, thereby mitigated the impact post which backlog data started flowing and which got cleared to resolve the issue.\n\n\n\nproblem\n\ninternal/external users are not able to search the forward curve symbols under forward curve charting under platts dimensions pro \n\nsolution\n\nthe cloudops team restarted linkerd and microservice deployments that injected linkerd proxy containers into the pods which helped in service-to-service communication resolved the issue.\n\n\n\nproblem\n\ndelay with ratings xpress v4 span files due to a job failure \n\nsolution\n\nratings xpress feed team identified that the autosys job (p_mi_xf_pullmerge_rx_core instrument revenue source) failed on sql server (rxf01dbsqlprd\\rxf) as one of the record (charter school) in sectorid inactivated caused this issue. pss team ran the autosys job to mitigate the impact\n\n\n\nproblem\n\nissue with us tagging server as elastic search service was consuming high cpu utilization. \n\nsolution\n\ntechops increased java space memory on the us tagging server from 1gb to 6gb (emergency cr chg0326576).\n\n\n\nproblem\n\nthe spglobal site that hosts crown peak pages was not accessible. \n\nsolution\n\nthe spglobal.com site that hosts the crown peak pages are not accessible and multiple technical teams tested the application servers and refreshed the load balancers, but the issue persisted. Crown peak (vendor) has been engaged with the case 83382. the crown peak vendor team verified that inbound traffic to the site was stable throughout the day and did not see any issues. in addition, the crown peak vendor team have performed multiple iis (internet information services) restarts, but the issue continues to persist. he crown peak support team was able to mount the backups and reconfigure microsoft iis to server to reference the backup disks instead of waiting for the completion of full copy/restore to the server disk. the site came back up, after restarting iis and has been stable without any site interruptions in the eu-west pod. the us-east pod was also brought into the rotation, with the same mounted drive and ratings site pages were validated and working as expected after mounting to the drive. this configuration will remain in place until the restore of content to the actual server drives and there will be a content publishing freeze to the sites until full restoration is completed on. crown peak vendor team is going to built a script to incrementally run though the published items from the last two days in small batches while monitoring the performance of the site. teams confirmed that publishing scripts has", "doc_id": "dafbbfc1-48c8-4e4b-98b6-51552acfd11a", "embedding": null, "doc_hash": "7bfcb665972e5dacd1b23f65b680af2882063b3c359e4f1bffbe5d0059705f6c", "extra_info": null, "node_info": {"start": 7873, "end": 12022, "_node_type": "1"}, "relationships": {"1": "4632de4d-db37-491c-b70d-3e7740dfdae5", "2": "6351959d-3b85-443e-a376-5419af675ef4", "3": "d842ad18-e5d0-4f2d-ab42-96e701ad2373"}}, "__type__": "1"}, "d842ad18-e5d0-4f2d-ab42-96e701ad2373": {"__data__": {"text": "small batches while monitoring the performance of the site. teams confirmed that publishing scripts has been completed and they have not received any further alerts from the site. the site is stable and fully back on the original disks with updated content. initial validation of all content seems to be functioning as expected. teams to conduct further in depth testing on to ensure all other content sourced from automated processes has been accounted.\n\n\n\nproblem\n\nratings users were unable to save/upload files in box. \n\nsolution\n\nbox support team had raised a case with the vendor (box case 2836213) network security raised a zscalar sev1case 04076495. network security team has bypassed all box urls from proxy and added a firewall rule with port 443 to mitigate the impact for few locations (japan, australia, hongkong, singapore, sydney melbourne). (emergency change - chg0324448). few users were still facing issue. box vendor team confirmed that ip address (107.152.29.224) was decommissioned and should not be in use anymore and suspect that this may be the cause. network security team has raised an emergency change (chg0325030) to disable invalid ip from box for few of the apac locations (to check if impact gets mitigated. as per box vendor recommendation the ip is a part of /20 prefix, the emergency change (chg0325030) implemented by n/s team to disable invalid ip from box on has been reverted (107.152.29.0/24 is re-enabled). box vendor has confirmed that they had stopped serving the content / cdn traffic since last year aug 2022. network data network security teams identified that the ad servers are forwarding requests from server (upload.app.box.com) to singapore infoblox dns forwarder (sg07ibx105.mhf.mhc infoblox) and it is returning to decommissioned box ip (107.152.29.224). network data team raised a case with vendor (infoblox case 00388287). box vendor team has confirmed that they have identified ns1 was returning incorrect dns in some cases and hence they deployed a fix to resolve the issue eams validated with users from all the apac region who are currently working from office and got the confirmation that they are able to save/upload file on box.\n\n\n\nproblem\n\ncontent users were getting error message as exception in closing the job while committing documents in foreseer application due to server application pool being stuck/flooded \n\nsolution\n\nmultiple teams (dse team, wintel team, extraction a team) joined the call to investigate the issue. dse team checked and confirmed that there is no issue from database side. extraction a team suspected that there was an issue with server application pool (ii48isp5001/ii48isp5002) which got flooded. mi content sre team recycled the app pool to resolve the issue. users have validated and confirmed the issue is resolved.\n\n\n\nproblem\n\nintermittent access issues on capital iq plugin on ciq classic platform due to tempdb contention on the oslo sql database server \n\nsolution\n\nmultiple support teams (dse, ciq sre, and storage) was engaged on the call to investigate the issue. ciq sre team recycled the app pool on the web servers - (qtciqwbpd01-08). dse team identified a tempdb contention on the oslo sql database server. dse team restarted the oslo sql server to mitigate the issue  validations have been performed and confirmed that the issue has been mitigated.\n\n\n\nproblem\n\nfew placeholders are randomly copying rap links in case of create placeholder (delete placeholder already handled) \n\nsolution\n\nwe have applied a fix to rebuild workspace placeholder list afresh from database whenever a placeholder is added\n\n\n\nproblem\n\nspan files were delayed on ratings xpress v4 due to sql database server constraints. \n\nsolution\n\nit was identified that span files was delayed on ratings xpress v4. ratings xpress team identified that sql database server (rxf01dbsqlprd\\rxf) had two strings (louisiana, la appropriation service contract p3 project) and (louisiana, la appropriation service", "doc_id": "d842ad18-e5d0-4f2d-ab42-96e701ad2373", "embedding": null, "doc_hash": "1bed2532a71dcc8d9d8094756eb8b8d5d5ee8905a18ac005c03e1ff773117018", "extra_info": null, "node_info": {"start": 12001, "end": 15975, "_node_type": "1"}, "relationships": {"1": "4632de4d-db37-491c-b70d-3e7740dfdae5", "2": "dafbbfc1-48c8-4e4b-98b6-51552acfd11a", "3": "b66d45d2-9202-450c-b651-c9c52cf80a1d"}}, "__type__": "1"}, "b66d45d2-9202-450c-b651-c9c52cf80a1d": {"__data__": {"text": "la appropriation service contract p3 project) and (louisiana, la appropriation service contract p3 project) that two strings acted as a same string which caused this issue. ratings xpress feed team was restarted the database job (p_mi_xf_pullmerge_rx_core instrument revenues ource) on the sql server which issue got mitigated no further clients cases has been reported so far validations have been successfully completed with the user and confirmed that the issue has been resolved.\n\n\n\nproblem\n\nmultiple functionalities in ciq pro platform experienced degradation on us-east stacks due to huge influx of calls from portfolio analytics via data api services saturating the system. \n\nsolution\n\nit was reported that multiple critical functionalities (company profiles pages and docviewer) experienced performance degradation on us-east (stack 1 and 2). ciq sre team moved traffic from us east to us-west to mitigate upon further investigation, ciq sre team along with desktop architecture team identified that portfolio analytics made large number of calls via data api services, which overwhelmed the system caused this issue. performance qa team performed load test on us east stack 2. since, services seemed stable, ciq sre team put back us east into the mix to resolve the issue\n\n\n\nproblem\n\ncontent users experienced slowness while performing activities in compustat data collection application (frde) due to high cpu utilization on few of the citrix prod servers \n\nsolution\n\nthe sybase processes were killed, and the impacted citrix servers were restarted, however there was no improvement. wintel support has doubled the capacity of the virtual cpus for the citrix servers in production to mitigate the issue.\n\n\n\nproblem\n\nblue prism automation bots were down due to connectivity loss between azure and on-prem vs load balancer. \n\nsolution\n\nnetwork security team confirmed there was connectivity loss between azure on-prem virtual machines which resulted in blue prism automation bots going down as a part of change (chg0315835-firewall upgrade), after rebooting the firewall, bgp session was cleared and switch took other path snldmvpn on firewall (ii48irt015) network data team modified the route configuration on the core switch (ii48cd101) and diverted traffic to firewall ii48tfw to mitigate\n\n\n\nproblem\n\naddiing team members to release team / post release workbasket issue \n\nsolution\n\n1) release team - added missing records in work group table for this team (table name: simplifydata74.pr_data_admin), these records insert from pega designer studio class instances. 2) post release workbasket - added missing records in region hierarchy business hierarchy table.( table names: simplifydata74.sp_team_region_hierarchy simplifydata74.sp_team_business_hierarchy )\n\n\n\nproblem\n\nissues with pipeline during db patching exercise that caused east db patching to be put on hold \n\nsolution\n\nspcom failover ansible code failed as the dependent aws repo branch it was referring to was missing in github. sp-ratings-ansible-aws repo was using dmz-spcom-redesign-no-proxy-failover which got deleted during branch cleanup. issue is fixed with new branch creation and updated changes. new branch is tested on alpha failover is working fine. \n\n\n\nproblem\n\nunable to create hierarchy review case when length of entity name exceeds 64 chars and special characters. \n\nsolution\n\ndue to field length discrepancy error was triggered. field length is increased and made consistent\n\n\n\nproblem\n\nfilings accumulating in extraction service on foreseer application due to requests not getting pushed to active messaging queue. \n\nsolution\n\nfundamentals team reported that they are seeing filings accumulating in extraction service on foreseer application. im-dt-fundamentals-transformation-dev confirmed that requests were not getting pushed to active mq (active messaging queue). dt foreseer team restarted the active mq to mitigate the impact and new filings started processing. im-dt-fundamental-transformation-dev team completed the reprocessing of the filings previously accumulated before the", "doc_id": "b66d45d2-9202-450c-b651-c9c52cf80a1d", "embedding": null, "doc_hash": "20910a2c040053c7c6a7f06513058d3ae7b28730fd7080f2fd4ae26d20967a4b", "extra_info": null, "node_info": {"start": 15989, "end": 20065, "_node_type": "1"}, "relationships": {"1": "4632de4d-db37-491c-b70d-3e7740dfdae5", "2": "d842ad18-e5d0-4f2d-ab42-96e701ad2373", "3": "97ac72b1-4dfa-47b4-8912-90b5d2972d8b"}}, "__type__": "1"}, "97ac72b1-4dfa-47b4-8912-90b5d2972d8b": {"__data__": {"text": "team completed the reprocessing of the filings previously accumulated before the fix was applied. validations are successful.\n\n\n\nproblem\n\nrating advisor rro submission created a lead not an opportunity \n\nsolution\n\nnow we are sending the user selected entity in case of ratings advisory when user search any rated issuer in the search box and storing in session storage \n\n\n\nproblem\n\nakkurat ll fonts are missing in cap windows servers \n\nsolution\n\nwe suspect this font installation done through sharepath and it deleted after server restart. however we have redeployed fonts again in production servers as part of april release and monitoring also in place hence resolving this.\n\n\n\nproblem\n\nannual review cra3 article publications \n\nsolution\n\nwe have removed the where condition check to related to cra3 at the denodo level and fetching the entire data and then written the logic to filter out the instrument related to cra3 entity. so it will remove both parent and child entities related to that cra3. fix is deployed to qa and uat environment. fix is tested and certified by qa. we have done uat with users and got the signoff.\n\n\n\nproblem\n\nremove black jpcr from pcr list on snp.com \n\nsolution\n\nthis issue will be handled as part of the jpcr republishing feature project (feature 5814962). we will ensure that when an entity in japan has both a financial enhancement rating (fer) and an issuer credit rating (icr) with a jpcr (jr indicator y ), the system identifies the fer as an entity-level rating, and that when a financial enhancement rating (fer) is processed, the system does not consider this as an issue rating.\n\n\n\nproblem\n\nvariable section data missing in da \n\nsolution\n\ndsp service should be migrated to kong\n\n\n\nproblem\n\nupon deletion of older ra job, the new job created does not refresh when opened \n\nsolution\n\nwe have made the changes in code so as to get the updated status for the jobs. once the old ra-job is cancelled and its status it set to cancelled, the updated status will now be mapped on the page instead of mapping the old status of the job. called the activity in step 1 of ruleset sprtg:18-04-90. call rtg-work.validateduplicateorgandrole and comment off the step 2.1.4 to avoid the double time property set.\n\n\n\nproblem\n\nas simplify is observing that there is change in the existing and recommended scores as we are fetching different existing scores in the rap and simplify. simplify system is blocking the user due to th \n\nsolution\n\nfixes done from rap side: updating input and output codes: a) need to update label and melabel for business_position business_position in model-scenario-lovs.const.ts file summary section: a) need to change the displaying logic for business and risk position values business position area: a) need to change the input mnemonic code and lovs data point b) need to change the output mnemonic code c) need to change highlighting the change logic d) need to change the required for sacp calculation logic e) on change, need to change update model engine value logic(in updatewithmodelenginedata method) risk position area: a) need to change the input mnemonic code and lovs data point b) need to change the output mnemonic code c) need to change highlighting the change logic d) need to change the required for sacp calculation logic e) on change, need to change update model engine value logic(updatewithmodelenginedata method) model engine area: a) need to update the melabels in model-scenario-lovs.const.ts to reflect the input values chagnes b) need to chage the logic to update the model engine output values(in setmodelengineoutputdata method) c) reset model output values, need to change the logic to reflect the changes(in resetmodelengineoutvaluesonui method) list view: a) need to update the mnemonic codes to reflect the new changes ui - test cases: a) need to updated few test cases in model-scenario-sample.component.spec.ts to work with the new changes observations in lov data(si, qa uat): for business_position_assessment, risk_position_assessment, the", "doc_id": "97ac72b1-4dfa-47b4-8912-90b5d2972d8b", "embedding": null, "doc_hash": "d9b4ef8f77ef81133ddf69fe8c7482cad5359f4f8ab0f0f627d172897e3b0fb0", "extra_info": null, "node_info": {"start": 20072, "end": 24101, "_node_type": "1"}, "relationships": {"1": "4632de4d-db37-491c-b70d-3e7740dfdae5", "2": "b66d45d2-9202-450c-b651-c9c52cf80a1d", "3": "e9cbbd90-0a78-4555-b8c7-ef29177cca7e"}}, "__type__": "1"}, "e9cbbd90-0a78-4555-b8c7-ef29177cca7e": {"__data__": {"text": "qa uat): for business_position_assessment, risk_position_assessment, the very strong lov value coming without space in between\n\n\n\nproblem\n\nsearch for object cannot be performed when there are special characters in entity. \n\nsolution\n\nwhen calling search object service pega will be passing escape character for the service to be able to parse when there is a double quote character in the search string\n\n\n\nproblem\n\nnew entry to add list of organization that have recevied a res or ca. \n\nsolution\n\nas per sop ops team will update res/ca org invoice dates in rpm and mongo db.\n\n\n\nproblem\n\nrpm does not let me attach documents \n\nsolution\n\nas part of the fix, we are indicating the user to enter up to 64 characters only and ensuring that user can only enter within the limit in the subject field available in the rl job attachment popup \n\n\n\nproblem\n\ncdo evaluator is missing from the models page \n\nsolution\n\narchiving newly created test version, model is displayed on r360.\n\n\n\nproblem\n\nnew bicra template is not pulling the scores into ramp \n\nsolution\n\nsimplify will pass cap a flag saying iti is birca org, cap will pass that flag to data service, data service will reverse lookup the id and get the data to come back in the service response. data will then flow to the ramp tables.\n\n\n\nproblem\n\nlog4j vulnerability - impacting multiple dmz and internal applications \n\nsolution\n\nteam continue to address log4j vulnerabilities that have evolved since the initial identification in december 2021. we are taking steps to ensure that teams prioritize updates to address these vulnerabilities through cab, ado pipelines and exception logging. the remediation of additional log4j vulnerabilities will be moved into bau support and there will likely be renewed office hours around vulnerability support overall to express the need for teams to keep their log4j files updated. there are exceptions in place for some apps that could not make the required changes. those teams will need to ensure those exceptions are maintained and not expired and renewed where necessary.\n\n\n\nproblem\n\nenable ros team to publish/extract jpcr for ticket reduction. \n\nsolution\n\nbuild the functionality to provide japan ros the ability to perform the following: 1) extract jpcr xml 2) upload jpcr xml 3) view amended pcr in pcr preview of rpm job 4) republish jpcr with amended xml\n\n\n\nproblem\n\nmultiple users were unable to launch applications hosted on citrix workspace \n\nsolution\n\nmultiple teams (citrix, network data, network security, aws cloud) were engaged. network data team found that the issue is not related to zpa since the applications are internet based and hosted on citrix cloud. as part of troubleshooting, network data team disabled internet security (zia proxy) option in zscaler application (zcc), post which the applications were launching. network security team raised a sev p1 case (03796236) with zscaler vendor to troubleshoot t proxy level. wireshark logs were captured and shared with the vendor (zscaler). network security team added url (spglobal.cloud.com) in zscaler app connector and updated the policy for few users, as a workaround recommended by vendor (zscaler), but the issue persisted. network security team has hitelisted few more urls suggested by citrix vendor, but the issue remained same. wireshark logs were again captured by zscaler vendor for further anlaysis. network security team then bypassed added spglobal.cloud.com zscaler portal windows app profile to mitigate the\n\n\n\nproblem\n\ncontent users are unable to open company profiles in celsus. \n\nsolution\n\ndse team identified blocking on database server (qtcol01dbsqlpd\\qtcol01dbsqlpd) due to a long running user query from server (qtmsctsin011). dse team killed the user query to resolve the issue.\n\n\n\nproblem\n\nauto article instantiation fails \n\nsolution\n\nupdated java certificate file by importing required server certificatie.\n\n\n\nproblem\n\nciq site was running extremely slow and while", "doc_id": "e9cbbd90-0a78-4555-b8c7-ef29177cca7e", "embedding": null, "doc_hash": "9f6929d1d9ae85ee632076cb1ffc6e346ac50221f57bff585266831d26f7c96b", "extra_info": null, "node_info": {"start": 24105, "end": 28057, "_node_type": "1"}, "relationships": {"1": "4632de4d-db37-491c-b70d-3e7740dfdae5", "2": "97ac72b1-4dfa-47b4-8912-90b5d2972d8b", "3": "00c63747-3875-45ca-9919-555bc1869ff8"}}, "__type__": "1"}, "00c63747-3875-45ca-9919-555bc1869ff8": {"__data__": {"text": "certificatie.\n\n\n\nproblem\n\nciq site was running extremely slow and while searching for companies or data set an error message was received. \n\nsolution\n\nthe issue is resolved by restarting the zookeeper nodes. however the soa team is investigating with the application team on improving the resource levels and configuration to provide better stability and availability. the work has started with the current sprint 22.02.06 and we will close the open items asap.\n\n\n\nproblem\n\nusers reporting multiple issues with pathfinder \n\nsolution\n\nlog backups were not happening as the backup preference was set to secondary as a result the log file got full on the primary database. changed the backup preference to any replica to take the log backups on the primary. we do not anticipate these issues again.\n\n\n\nproblem\n\nusers are unable to access docshare links which are share via email once the jobs is released \n\nsolution\n\nwe have replaced the functionality by using insight protocol with chronicle id of the document. this will eliminate the dependency on the document location. change is moved verified by qa and deployed in prod on \n\nproblem\n\nusers are experiencing intermittent login issues on ciqpro \n\nsolution\n\nworked with eps (end point security) team and whitelisted folders. additional trace flags added to mute dump generation while ms fixes bug in product as part of sql server 2019 cu16.\n\n\n\nproblem\n\ndata loader service not running daily as per schedule automatically. \n\nsolution\n\ncdc processor was running in the streaming cluster. now, its changed to job cluster\n\nproblem\n\nuser requested access to apac, emea and us templates \n\nsolution\n\nthis was a user access issue and the user was re-directed to the arp team who were able to resolve the issue.\n\n\n\nproblem\n\n973491- not allowing icr rl to be drafted; job cannot be sent to billing \n\nsolution\n\nenabled the logs to identify the issue on next occurrence and logs are planned to move production.\n\n\n\nproblem\n\nunable to move forward on ratings screen due to edc. \n\nsolution\n\nmade logical fix when edc - when na - not applicable option is selected for edc, this na value tag is not sent to core at all, as the tag is not sent to core it does not validate anything and allow case to process. alpha names will bypass the validation stage. this is a permanent fix, deployed via chg0250571\n\n\n\nproblem\n\nr360 - missing performance data on ui -(timing issue) \n\nsolution\n\nthe missed ids have been reprocessed and verified in r360 ui. after reprocessing the data is available on r360 ui.\n\n\n\nproblem\n\ndr deployment was not 100% successful for rap-servingdeployerwin-caas \n\nsolution\n\ncorrected the host name in the yaml file and added the missing key rap_oauth_token_username\n\n\n\nproblem\n\nuser transferred from mi to ratings and caused profile issues/multiple names in system \n\nsolution\n\nthere is now a feed from sailpoint to arp - there is additional work being undertaken to automate tasks from the sailpoint\n\n\n\nproblem\n\n502 bad gateway error while creating data source for the v_pw_performance view in immuta \n\nsolution\n\nthe issue has been resolved after upgrading immuta to 2021.5. now, the data source is being created and the view can be queried from reporting cluster.\n\n\n\nproblem\n\nunable to load documents in simplify case. \n\nsolution\n\nas part of child requestor initiation process we are verifying the run time context and resetting the context if it is getting the wrong context. context will reset to simplify after rpm and wfm are used, instead of leaving it in the most recent state. if multiple applications (rpm, wfm and simplify are opened at the same time, simplify will be prioritized. attached the change to the prb\n\n\n\nproblem\n\nunable to execute python commands in databricks notebook resulting in timeout error \n\nsolution\n\nits server restart. no permanent fix needed\n\n\n\nproblem\n\nwhen signoff analyst tries to signoff an ar, takes them to rpm screen", "doc_id": "00c63747-3875-45ca-9919-555bc1869ff8", "embedding": null, "doc_hash": "2d81865b6663c497c47ef6856d20359bc2a786e3203c66a4afa2b234b6660541", "extra_info": null, "node_info": {"start": 28061, "end": 31966, "_node_type": "1"}, "relationships": {"1": "4632de4d-db37-491c-b70d-3e7740dfdae5", "2": "e9cbbd90-0a78-4555-b8c7-ef29177cca7e", "3": "abfeea92-acd0-41a9-b116-7b5f5eb2899c"}}, "__type__": "1"}, "abfeea92-acd0-41a9-b116-7b5f5eb2899c": {"__data__": {"text": "signoff analyst tries to signoff an ar, takes them to rpm screen \n\nsolution\n\npost updating right endpoint url cases are now routed to correct application simplify\n\n\n\nproblem\n\nmultiple content applications were not working as expected on both local and remote apps. the snltxprocess memory leak issue. \n\nsolution\n\ncompleted- replaced that code with a method using native c# and the memory leak when away. this was rolled out to production.\n\n\n\nproblem\n\nexcel files in docshare are opening only in read-only format. \n\nsolution\n\nexcel files read only issue was fixed by updating the configuration files in docshare servers with emergency cr chg0219778\n\n\n\nproblem\n\nincorrect date showing up in the reports section \n\nsolution\n\nupdated date format provided through code fix.\n\n\n\nproblem\n\nintermittent login issues on cflow \n\nsolution\n\ntechnical teams restarted the cflow webgate servers in a rolling fashion following which users confirmed they could log in to the application consistently.\n\n\n\nproblem\n\nconstituent characteristics are not showing for newly launched indices. \n\nsolution\n\nuser training issue - no corrective action is needed -\n\n\n\nproblem\n\ncompustat files are not uploading to edx platform due to couple of job failures. \n\nsolution\n\nthe access keys have been updated to the latest ones.\n\n\n\nproblem\n\nautomated health checks were failing for https://www.ratefilings.com \n\nsolution\n\nwe have updated the configuration so that the service always picks up the latest installed version on the machine.\n\n\n\nproblem\n\nedx api service was not working due to an issue with the resolution of vip (https://soaapi.ws.spglobal.com/edx_prod_spg/). \n\nsolution\n\nsoa team suspected there was an issue with soa vip. soa team had rolled back the new certificate (ssl), suspecting it to be the cause. this certificate was installed, issue persisted. network security had confirmed that the external client facing soa load balancer was fine, and the inbound connections were being forwarded to the soa backend servers. edx team had confirmed that the edx backend servers were good. soa team has failed over the traffic from secaucus to ashburn to mitigate the issue.\n\n\n\nproblem\n\ndelay in xpressfeed, ratingsxpress v3 and v4 files. \n\nsolution\n\ninc3086614 - network team identified that there was a network blip for 6 minutes. jobs were restarted. inc3161299 -pss team restarted these jobs to fix the issue\n\n\n\nproblem\n\nemail notification of cmt ramp points to rpm but loads nothing \n\nsolution\n\nnow right endpoint urls been aligned to simplify application\n\n\n\nproblem\n\nbug: 4688750: customer cannot create an arp request due to enterprise policy role missing \n\nsolution\n\nthere were two fixes associated to this issue. 1) additional daily synchronization was instituted by the opdm team - synchronization will run between and opdm (9 am and 5 pm) to allow the system to update changes that come in after 9 am so that the customer is not left waiting a full 24 hours for synchronizations to complete. secondary change was on the sailpoint side to address the following issue- arp acrep default policy role is not triggering. this was fixed on 2/25 through change control.\n\n\n\nproblem\n\nads not finalized (fr and edr) \n\nsolution\n\nfix implemented: rule base mismatch with the service request- we updated existing service to send the correct rulebase in the rulesets, thus enabling simplify to consume this data and trigger the closure of the y case. code fix deployed to prod on 18th march 2022.\n\n\n\nproblem\n\nrating letter job upload issue - rls are replaced by another rl from a different deal \n\nsolution\n\ncorrected the code to stop tagging incorrect jobs\n\n\n\nproblem\n\nsecurity ids expected to appear on ratings360 clo collateral universe \n\nsolution\n\nthis particular issue is due to the timing and this has been taken care as part of containerization (cho data center migration) of data pipeline solutions.\n\n\n\nproblem\n\nfew of the capitaliq.com platform", "doc_id": "abfeea92-acd0-41a9-b116-7b5f5eb2899c", "embedding": null, "doc_hash": "261d3d9f58cc9e7253e83d377f974bd25fa140e1503bead67708ecb5bbc00cda", "extra_info": null, "node_info": {"start": 31978, "end": 35900, "_node_type": "1"}, "relationships": {"1": "4632de4d-db37-491c-b70d-3e7740dfdae5", "2": "00c63747-3875-45ca-9919-555bc1869ff8", "3": "2b07d91f-1c85-4289-a7ba-30cddd3958a1"}}, "__type__": "1"}, "2b07d91f-1c85-4289-a7ba-30cddd3958a1": {"__data__": {"text": "pipeline solutions.\n\n\n\nproblem\n\nfew of the capitaliq.com platform functionalities are impacted \n\nsolution\n\nsubsequent patches to clustered instances were done manually.\n\n\n\nproblem\n\nadditional rating in pvt rating_ra-284704 mismatch in history \n\nsolution\n\nrevised the sort logic for regulation l of the u.s. pcr (historical performance of credit rating) for private ratings so the rating history table is sorted properly. this was released. in addition, ra-284704 where the issue was first identified as been corrected by ops.\n\n\n\nproblem\n\nuser was seeing an error message while trying to access gre. clearing browser cache resolved the issue. \n\nsolution\n\nuser cleared the browser cache which resolved the issue and the application started functioning as expected.\n\n\n\nproblem\n\nthere is a ftp connection issue due to which the data did not flow down from cs to finmaster. \n\nsolution\n\nwe have requested creditscope team not to rebuild or apply any patches with re-start on business days to avoid these failures going forward. also including consumers for the communication incase of rebuild happening in urgency and the latency i.e is expected. this will be fully resolved once creditscope is fully retired. additionally we will review the notification dls to be used for such maintenance notifications so if there is a need to let additional consumers know of such unplanned activities, we will include those notification dls in the process.\n\n\n\nproblem\n\nhogging thread / high cpu load average in prod app servers (stability) \n\nsolution\n\nas a part of stability initiative, rdr, docshare content servers were moved to new infrastructure and migrated from vm to ec2 instances. additionally the cosi client was upgraded upgraded to 6.4 version. related change from 10/14 release is attached.\n\n\n\nproblem\n\ncrisis : aar2-overnight process to calculate and refresh analyst rotation data failed to complete successfully \n\nsolution\n\nemergency change (chg0218666) was executed to address the field size of rotation_cycle field in the aar2.rotation table. it was increased from 1 byte to 2 bytes to accommodate double digit rotations (greater than 9). this change was identified as a workaround pending additional review of upper limits and determination if there is a need to accommodate any rotation cycles above 99. if the max limit of 99 is sufficient after analysis, then this change will be considered a permanent fix.\n\n\n\nproblem\n\nzscaler service status is throwing error and im unable to connect to vpn. \n\nsolution\n\nthe zscaler client connector (zcc) agent version was upgraded from 3.5.0.108 to 3.6.1.25 based on the recommendation from zscaler to improve the performance of zscaler internet proxy (zia) and zscaler private access (zpa) services.\n\n\n\nproblem\n\nhybrids : issues with caching process, which went down making application unusable \n\nsolution\n\nwipro changed the permissions\n\n\n\nproblem\n\nunknown macro errors while accessing some of the platts wiki pages \n\nsolution\n\nin order to resolve the issue, the wiki site was taken offline and each prod node was started one-by-one in order to re-sync settings between all nodes\n\n\n\nproblem\n\nids and dot cms are down and showing 502 error. \n\nsolution\n\nimmediate resolution\n\ntomcat restart was done to resolve the issue during the incident period. permanent resolution\n\n1. chg0168632 ( - update target groups prod-ids-web-tg-04 and dr-ids-web-tg-04. change deregistration delay from 60 secs to 600 secs for target groups prod-ids-web-tg-04 and dr-ids-web-tg-04. deregistration delay will give enough time to complete such requests and will help reduce 502 errors. 2. chg0169205 ( - urgent ids production release v_21_2_3 to resolve to resolve 5xx errors ado: 3831551 ttps://spglobal.visualstudio.com/spdji/_workitems/edit/3831551 1. to avoid dotcms container recycling when response time becomes", "doc_id": "2b07d91f-1c85-4289-a7ba-30cddd3958a1", "embedding": null, "doc_hash": "9acb8c891ba665e870a66e89e03802add3e779184a7a134b540bb7a28d19ad1c", "extra_info": null, "node_info": {"start": 35894, "end": 39728, "_node_type": "1"}, "relationships": {"1": "4632de4d-db37-491c-b70d-3e7740dfdae5", "2": "abfeea92-acd0-41a9-b116-7b5f5eb2899c", "3": "f0c8453c-e8fe-44e9-a622-2a028b642c30"}}, "__type__": "1"}, "f0c8453c-e8fe-44e9-a622-2a028b642c30": {"__data__": {"text": "1. to avoid dotcms container recycling when response time becomes high 2. to complete the ecs migration of the apache (part 1 implemented in chg0130317) below are the changes implemented. 1)\tincrease db connections from 150 to 300 2)\tchange jvm memory parameters (move from initial percentage to xmx option) 3)\ttomcat catalina output logging to console 4) apply oracle recommendations to cmsp db. 5)\tapache and apache maintenance logging to console 6)\tenable aws es snapshot and restore for ids\n\n\n\nproblem\n\nnews, ratings action, credit research and credit analytic alerts are down \n\nsolution\n\nsuccessfully, bring back the servers by restarting the brokers.\n\n\n\nproblem\n\nautosys: weekly symbol job (p_iftp_fc_symbol) got failed: delay in posting the weekly symbol files to the business partners \n\nsolution\n\netl weekly symbol job ran successfully after the iftp secondary node was shut back down, and read/write permissions were corrected on the archive directory.\n\n\n\nproblem\n\nratings major incident - can not save/check in documents \n\nsolution\n\nsession load balancer issue is fixed and deployed in to production. issue is with configurations and it has been worked with open text and cosi vendor to address the issue\n\n\n\nproblem\n\nmultiple nodes belongs to subnet 10.25.228.0 and 10.25.230.0 were inaccessible. \n\nsolution\n\nnetwork security team extended the bandwidth license to 5gbs per sec on f5 load balancer virtual device (va06bigipstrint1/2.mhf.mhc).\n\n\n\nproblem\n\nusers were facing intermittent connectivity issues on singapore vpn for internet based business applications. \n\nsolution\n\nnetwork security team confirmed that they do not have product license contract, which is expired. moved sg vpn users to us vpn. informed to engineering team to migrate sg vpn on zscaler.\n\n\n\nproblem\n\narrow global application is not accessible \n\nsolution\n\nmonit script startup was pointing to wrong script name,hence the managed server failed to come up automatically.\n\n\n\nproblem\n\nlatest research documents by contributors were not being published to the platform (capital iq desktop and spg platform) \n\nsolution\n\nssis jobs were not able to pick up configuration from config files. there was no change done in ciq release, but after ciq release, since ssis jobs were not able to pick up data from config files, we requested gdps to set the config values explicitly and that fixed the issue. it took some time for backlog to get cleared\n\n\n\nproblem\n\nratings major incident: cannot open gre filter \n\nsolution\n\nveda from middleware team has provided details on the infrastructure issue. from the application site we have migrated to caas infrastructure from ec2 so this issue will not happen again.\n\n\n\nproblem\n\nhi team, please can you add the the region (europe) to the pw job on the back end \n\nsolution\n\nfixed an issue where pw-jobs were not appearing in the business liason workbasket due to the user not selecting the region providing rating field prior to saving the job. users will now be required to select the region providing rating field prior to saving (or submitting) the job.\n\n\n\nproblem\n\nids 502 gateway error \n\nsolution\n\ninvestigation complete - fixes implemented for 502 errors immediate resolution\n\ntomcat restart was done to resolve the issue during the incident period. permanent resolution\n\n1. chg0168632 ( - update target groups prod-ids-web-tg-04 and dr-ids-web-tg-04. change deregistration delay from 60 secs to 600 secs for target groups prod-ids-web-tg-04 and dr-ids-web-tg-04. deregistration delay will give enough time to complete such requests and will help reduce 502 errors. 2. chg0169205 ( - urgent ids production release v_21_2_3 to resolve to resolve 5xx errors ado: 3831551 https://spglobal.visualstudio.com/spdji/_workitems/edit/3831551 1. to avoid dotcms container recycling when response time becomes high 2. to complete the", "doc_id": "f0c8453c-e8fe-44e9-a622-2a028b642c30", "embedding": null, "doc_hash": "f9d11148b9762f86891df5bded85bc3ddb7038a0439576d7928398cb19586e6f", "extra_info": null, "node_info": {"start": 39733, "end": 43573, "_node_type": "1"}, "relationships": {"1": "4632de4d-db37-491c-b70d-3e7740dfdae5", "2": "2b07d91f-1c85-4289-a7ba-30cddd3958a1", "3": "9be49005-2a1c-4530-8a69-f1ea20aa3c71"}}, "__type__": "1"}, "9be49005-2a1c-4530-8a69-f1ea20aa3c71": {"__data__": {"text": "1. to avoid dotcms container recycling when response time becomes high 2. to complete the ecs migration of the apache (part 1 implemented in chg0130317) below are the changes implemented. 1) increase db connections from 150 to 300 2) change jvm memory parameters (move from initial percentage to xmx option) 3) tomcat catalina output logging to console 4) apply oracle recommendations to cmsp db. 5) apache and apache maintenance logging to console 6) enable aws es snapshot and restore for ids ecs go-live part 2 1. update unhealthy threshold from 5 to 2 and timeout from 10 seconds to 5 seconds for the following target groups. update health check path to /health-checks/index.html prod-ids-web-tg-04 dr-ids-web-tg-04 2. update the dns - spindices.com to point to ecs cloudfront.spindices.com  prod-ids-apache-pub-alb-01-172732379.us-east-1.elb.amazonaws.com (ids-prod-primary weight 255) cloudfront.spindices.com  dr-ids-apache-pub-alb-01-1165569200.us-west-2.elb.amazonaws.com (ids-dr weight 0) lb.spindices.com  prod-ids-apache-pub-alb-01-172732379.us-east-1.elb.amazonaws.com (ids-cp-prod-primary weight 255) lb.spindices.com  dr-ids-apache-pub-alb-01-1165569200.us-west-2.elb.amazonaws.com (ids-cp-dr weight 0) 3. make sure dr-ids-apache-pub-alb-01 has proper waf(you can compare it to dr1-ids-apache-alb). update it if required.\n\n\n\nproblem\n\nheards were not updating in pmc, platts platform and eclipse \n\nsolution\n\npas and dev team worked together to clear the filtered content queue on prp sonic server restarted the sonic services, marklogic dbops team manually triggered the messages which were not triggered.\n\n\n\nproblem\n\nchange dg title and ensure dg is also added to another section--needed on rpm, wfm and simpifly \n\nsolution\n\nto comply with regulatory requirements and to align with the publishing of the social housing providers (shp) criteria, disclosure group uspf: revenue enterprise debt was renamed as uspf/ ipf: enterprise and revenue debt. the revised name will appear in the sector-based disclosure group dropdown of rpm, in between the sovipf and uspf disclosure groups. when this disclosure group is selected for either uspf or sov/ipf, the existing disclosure text will be displayed in the pcr.\n\n\n\nproblem\n\nusers were unable to access older files in shared drive \\\\ny01fil500\\ \n\nsolution\n\nwipro storage team had rehydrated the archives files back to the primary storage to avoid causing recurrence issues and decommission the eol archive owned and managed by vendor emc/cdc.\n\n\n\nproblem\n\nr360 : sovereign : financial sector tab is not loading data due to race condition issue between dsmp and rdsh generating empty payload \n\nsolution\n\nthe race condition in spscores_ref.proc_scores_ack that dsmp event was committed while the ack status hadn t. the commit is at the very end while dsmp insertion is before the commit. this can be easily fixed by calling the dsmp.pkg_event_queue.insrt_event_ext_trans_control instead of the dsmp.pkg_event_queue.insrt_event_info. give the commit control to the caller.\n\n\n\nproblem\n\nusers were facing edx slowness issues for qts and ashburn. \n\nsolution\n\nissue was fixed by edx failover to ashburn and then increasing tcp time out on ftps-edxprdqts.capiqinc.com from 5 min to 60 mins\n\n\n\nproblem\n\nsnl.com website was intermittently failing to resolve from multiple locations globally. \n\nsolution\n\nting vendor informed they are working with their existing vendors to improve ddos", "doc_id": "9be49005-2a1c-4530-8a69-f1ea20aa3c71", "embedding": null, "doc_hash": "8c0e8aef2cde7b053b9a98058f24f1ae169abc15969e929bcfe92f58f184ff1f", "extra_info": null, "node_info": {"start": 43555, "end": 46987, "_node_type": "1"}, "relationships": {"1": "4632de4d-db37-491c-b70d-3e7740dfdae5", "2": "f0c8453c-e8fe-44e9-a622-2a028b642c30", "3": "9b403986-af9b-4a5f-9776-f93b30d8e2da"}}, "__type__": "1"}, "9b403986-af9b-4a5f-9776-f93b30d8e2da": {"__data__": {"text": "vendor informed they are working with their existing vendors to improve ddos mitigation services to address and rectify ddos attacks in a timely manner also revise their architecture to further improve the overall security against malicious traffic.\n\n\n\nproblem\n\nsimplify system is not working in production as we expected for backup team leads when reassigning cases \n\nsolution\n\nissue has been fixed and now all the back up team lead can edit manage assignment and save data as per need. before code fix, there was a glitch on existing code where application is able to find only 1 back up team member and that to be who is the 1st row member and discarding all other. after the code fix, now application can able to identify all the back team availability automatically and each back up team lead member can able to edit the presenting analyst or support analyst members.\n\n\n\nproblem\n\nusers were facing intermittent connectivity and slowness on serv-u (ftp.capitaliq.com and ftp.clarifi.com). \n\nsolution\n\nas per the vendor recommendations team have increased of cpu and memory from 2 cpu 4gb to 4 cpu 8 gb through chg0165722 to resolve the issue.\n\n\n\nproblem\n\nt3 used old weight rules in the system (with effective date of 20200922) for snp smit 40 \n\nsolution\n\nwe had a check in the system, system will fails the calculation on proforma date if index is rebalancing and weights are not uploaded as rebalance date for all the stocks which are part of the index, but this is broken in one of the old release, we identified it recently and fix is implemented as part of spdji\\release 21.07\\sprint 21.07.02\n\n\n\nproblem\n\nratings jobs are delayed on ratingsxpress v3 \n\nsolution\n\nnetwork got glitched some how and jobs got failed, we have restarted them accordingly.\n\n\n\nproblem\n\nportal logins are failing for production ciq platform \n\nsolution\n\nciq support team started the services manually on those servers post which teams confirmed that the portal login issue was fixed\n\n\n\nproblem\n\narticle service failing intermittently due to out of memory error \n\nsolution\n\nteam confirmed all problem tasks were completed and work was done to expand faas capacity to autoscale based on volume of data.\n\n\n\nproblem\n\nusers from different locations were unable to connect to aws workspaces \n\nsolution\n\nvendor aws support disabled the original update script for all prod aws. teams then renamed the pcoip agent update script to bypass the script execution and observed that the machines did not reboot automatically. the rename script powershell script then was executed to all the impacted systems, post this execution of script users had confirmed that they were able to login.\n\n\n\nproblem\n\nissues with hydra pages service on singapore cloud snp global platform. (mitigated) \n\nsolution\n\ndetect: we got datadog alerts for p95 for stack 2 singapore for hydra data service and also for high number of requests being stuck on as2soahdtp162 server. response - singapore stack 2 was pulled out of the mix to avoid any issues to the customers and the server as2soahdtp162 was rebooted to resolve issues. prevention- process to automatically pull the server from the mix if the number of requests on the server starts piling up\n\n\n\nproblem\n\nintermittent slowness issues with the sourcing tab in celsus application. \n\nsolution\n\nwe moved few apps am,oidmapping,msh uploader crc generation to point qtcolappd(05-06) servers instead of qtcolappd(01-04). it got reduced the load for celsus users and performance got increased.\n\n\n\nproblem\n\nnot receiving emails | nutley environment \n\nsolution\n\nwhen the content switch was reloaded functionality was restored.\n\n\n\nproblem\n\ngcc users were unable to create and publish news to mi platform \n\nsolution\n\nrebuilt indexes for articleindex table and added the table to automated stats updates process\n\n\n\nproblem\n\nhigh cpu utilization in iisfgr which result in db performance issue\n\nsolution\n\nwe are good after moving to new vm\n\n\n\nproblem\n\nlogout functionality was", "doc_id": "9b403986-af9b-4a5f-9776-f93b30d8e2da", "embedding": null, "doc_hash": "a54195808434df9b07c8e72df56f645156676866d0d1e1b77478d800957e4c45", "extra_info": null, "node_info": {"start": 46998, "end": 50963, "_node_type": "1"}, "relationships": {"1": "4632de4d-db37-491c-b70d-3e7740dfdae5", "2": "9be49005-2a1c-4530-8a69-f1ea20aa3c71", "3": "0a9a86e8-8305-4493-9040-3b1c41f9726b"}}, "__type__": "1"}, "0a9a86e8-8305-4493-9040-3b1c41f9726b": {"__data__": {"text": "are good after moving to new vm\n\n\n\nproblem\n\nlogout functionality was intermittently failing on capitaliq platform. \n\nsolution\n\nthis can be closed a permanent fix. as team took necessary steps as below - to ensure any routing changes should not be performed during the weekdays and should go through cab/tab approvals( ptask0017756). updated sop for emergency change (ptask0017755)\n\n\n\nproblem\n\nintermittent slowness on servu (ftp.clarifi.com and ftp.capitaliq.com). \n\nsolution\n\nwe identified resources crunch on servers. we requested wintel team to upgrade the ram from 4gb to 8 gb, it is an vmware wintel team upgraded the ram to 8 gb on file. contention issue was resolved.\n\n\n\nproblem\n\nmultiple iframe dependent applicaitons are not opening in chrome when launched through ratings gateway. \n\nsolution\n\nthe problem is fixed by following steps: 1. installation of webgate binaries which as suggested by oracle idm team has the binaries (and add a wg udf parameter) 2. added secure;samesitenone lines in apache extra.conf file. 3. cookie domain fix for pega\n\n\n\nproblem\n\nids site is down \n\nsolution\n\ndue to the large import done by web team, the ids site was inaccessible for 3 mins. after the rolling restart of ids tomcat servers, the ids nodes was healthy. users have been advised to perform bulk uploads at low traffic times and to space them out such that you can confirm the first one is complete and synced across nodes before starting the next one.\n\n\n\nproblem\n\ndocker images in artifactory preventing containers from starting \n\nsolution\n\nteam restored the images to artifactory and amended the scripts that were used as part of the cleanup process.\n\n\n\nproblem\n\nusers not able to submit documents through directconnect \n\nsolution\n\nserver side confi was updated and deployed to prod.\n\n\n\nproblem\n\nmismatch ratings on r360 \n\nsolution\n\nupdate the cache duration to zero seconds from 2 hours so that no delays on the ratings.\n\n\n\nproblem\n\nslowness and errors with ciq excel plugin and ciq web portal login. \n\nsolution\n\napi gateways have the connection count thread changed from 128 to 1000 and restarted the services\n\n\n\nproblem\n\nindustrial and commercial bank of china (new zealand) limited issuance ratings missing \n\nsolution\n\nupdated the hydrob to fix the issue and verified the fix on lower environments. it will be deployed to production as per the schedule.\n\n\n\nproblem\n\nciq portal and excel plugin idm 12c delays due to the ciq 12c webgate api gateways servicing the plugin calls exceeded limits \n\nsolution\n\non the api gateway servers we increased the cpu and memory resource by 2x and also increased the concurrent connection pool configuration by 4x to allow for more concurrency and reducing the thread saturation. \n\n\n\nproblem\n\ncreditpro experienced an outage. \n\nsolution\n\nthe issue was resolved. tanium scheduling was put in place.\n\n\n\nproblem\n\nusers are unable to see aphub search screen ( while login) \n\nsolution\n\nsearchemployee / getemployeedetailsby id service is returning huge response for general comments as result user is unable to see the complete aphub screen due to response time out. we have suppressed general comments in response while user logins into aphub application\n\n\n\nproblem\n\nprice metric was failing on charting and corporate profile on snp global platform. \n\nsolution\n\nthe issue was caused as an unexpected update was pushed onto the proc. the issue was resolved when the 4 procs where updated to the expected status by jason johnson from the rsa team.\n\n\n\nproblem\n\npipeline that brings commodity data from ciq platform to mi platform is down. \n\nsolution\n\nissue was resolved by re-processing the data and no more incidents of this type have been reported as of the end of 2021\n\n\n\nproblem\n\ncan not access https://neo.app.tricentis.com/ and https://vision-ai.app.tricentis.com/ in ie and tosca vision", "doc_id": "0a9a86e8-8305-4493-9040-3b1c41f9726b", "embedding": null, "doc_hash": "2bc46b5e8f96c3b230a6b88fc16c7c429f6e52b1c36897d65ef0e1dac22cd836", "extra_info": null, "node_info": {"start": 50969, "end": 54796, "_node_type": "1"}, "relationships": {"1": "4632de4d-db37-491c-b70d-3e7740dfdae5", "2": "9b403986-af9b-4a5f-9776-f93b30d8e2da", "3": "95f6aaa7-1b35-4200-b3de-75b233d37679"}}, "__type__": "1"}, "95f6aaa7-1b35-4200-b3de-75b233d37679": {"__data__": {"text": "in ie and tosca vision ai agent \n\nsolution\n\nthis issue was resolved when a package for the software installation was created and published to software center.\n\n\n\nproblem\n\ncap qa loading issue \n\nsolution\n\nquery fine tuning in qa and uat db fixed the issue\n\n\n\nproblem\n\ninformatica services were impacted in charlottesville dc \n\nsolution\n\nnetwork security team has applied the fix by reloading the network firewalls and double the ip pool capacity initial test results look positive\n\n\n\nproblem\n\nsql service was restarted on dmzsql3 \n\nsolution\n\ndmzsql3 was restarted due to heavy purge activity on onlinelogging.sessionstate table causing high threadpool count/active sessions/blockings. further analysis showed that the said table had high index fragmentatiom. gdps have performed index maintanence of that table through the chg0150921. post the index maintenance, we do not see any issues with dmzsql3.\n\n\n\nproblem\n\nparc transaction log is full due to replication \n\nsolution\n\nreplication was not working for telekursdata and the log reader agent had filled the transaction log up filling up the drive. i fixed replication and shrank the log. i also created a 2nd log for parc to prevent this from happening again.\n\n\n\nproblem\n\nmhf1 users are intermittently unable to access folders contents within shared drives \n\nsolution\n\nreported issues were resolved by reapplying or re-adding permissions domain. not reproducible anymore.\n\n\n\nproblem\n\nresearch articles for realtime ratings, rd on ciq and xpressfeed files were delayed. the impacted files from xpressfeed are rating v2,v3,v4 and credit research \n\nsolution\n\nthe issue started on qtdfpdbpd01 server got crashed due to that athens with golden gate services failed over to qtdfpdbpd02 from qtdfpdbpd01 but services not started automatically. manually started golden gate services on passive node qtdfpdbpd02 fixed the issues\n\n\n\nproblem\n\nfim connector showing empty values on simple access account creation for non employees. \n\nsolution\n\npermanent fix is deployed by:- - removing the offending dc (nj12adc001) on 23rd dec and updating the healthy ones - team has monitored this for three months and no issues found. there were accounts in queue that did not get auto processed, hence few issues were noticed exports are running fine via nj12adc003, auto provisioning of accounts is in place now.\n\n\n\nproblem\n\nplt eclipse xplore: latest outage data was not populated for ncs due to one tableau extract getting timedout \n\nsolution\n\nto mitigate the issue dbops team changed the execution to live connection of a single data source to get the realtime data.\n\n\n\nproblem\n\nusers working from manila and pune odc (spgi and ihs) were unable to access internet via lan due to a power failure on a legacy switch in secaucus dc \n\nsolution\n\nthe onsite engineer visited the datacenter and powered on the switch which mitigated the issue.\n\n\n\nproblem\n\nindexmetadatafeed - error occurred when validating remote object grip.index_dest_for_ids@gripxp \n\nsolution\n\ndv team have executed the arping command for the nutley dns\n\n\n\nproblem\n\nmeeting tab is showing as not entered for publishing job \n\nsolution\n\npermanent fix applied by removing hard coded values in meeting list and passing event to the function to call process action.\n\n\n\nproblem\n\ncase shiller data files not distributed to edx causing delays of data updates on ids \n\nsolution\n\nimpg team corrected the file names (added hash tag around the date part) and did an immediate posting of the data files from care to edx only.\n\n\n\nproblem\n\nmissing md from vendor \n\nsolution\n\nthis was fixed during fierce release 6.31.1 5818907 prb0062750 - missing md from vendor\n\n\n\nproblem\n\nhyperlinks are not working from outlook in citrix vdi. \n\nsolution\n\ncitrix vendor advised a registry workaround to add outlook.exe to exclusion list", "doc_id": "95f6aaa7-1b35-4200-b3de-75b233d37679", "embedding": null, "doc_hash": "5ffa1ebed1f7c7d3a6fd6b26b3aee1ba868d945d74977b84e9134baf116126b9", "extra_info": null, "node_info": {"start": 54839, "end": 58640, "_node_type": "1"}, "relationships": {"1": "4632de4d-db37-491c-b70d-3e7740dfdae5", "2": "0a9a86e8-8305-4493-9040-3b1c41f9726b", "3": "4667514a-1b33-42e0-b181-bdba9a4eadd4"}}, "__type__": "1"}, "4667514a-1b33-42e0-b181-bdba9a4eadd4": {"__data__": {"text": "vendor advised a registry workaround to add outlook.exe to exclusion list of citrix api hooks. so that the citrix api involvement of calling outlook.exe processes will be excluded and outlook.exe runs on its own processes to execute. tested the workaround by getting patching team deployed the microsoft patches on the vdis with registry added. hkey_local_machine\\system\\currentcontrolset\\services\\ctxuvi value name: uviprocessexcludestype: reg_szvalue: outlook.exe citrix team has deployed the registry to all the impacted vdi based on the user reports which resolved the issue.\n\n\n\nproblem\n\nsearch for object cannot be performed when there are special characters in entity. \n\nsolution\n\nwhen calling search object service pega will be passing escape character for the service to be able to parse when there is a double quote character in the search string cr: chg0323638 rwi: https://spglobal.visualstudio.com/ratings/_workitems/edit/5906140\n\n\n\nproblem\n\nmodels / ramp template / documents opening in rap eventhough its not integrated. \n\nsolution\n\nweve added auto refresh activity to the placeholders so that upon each deletion of a document/placeholder, the mapping is refreshed and that the new placeholder is mapped properly, it overrides the previous mapping details in simplify db.\n\n\n\nproblem\n\nerrors while saving in cts institution form and delay in delivery of latest company profile data to ciq pro platform due to middle tier release. \n\nsolution\n\nit was identified that the issue was caused due to shire api service was returning failed requests. shire api service started failing after new version of mts nuget was released middletier to support future functionality for non-transactional batches. mi dataservices bteam stated that this functionality should not have had consumers in production. however, because shire was incorrectly using non-transactional batches, therefore application started attempting to use this incomplete functionality. mi dataservices b team upgraded the mts nuget to support non-transactional requests and deployed it in lower environment for testing. validations were unsuccessful in staging and it was identified that queries are even failing with the latest version of mts nuget in staging and expected to fail in prodcution as well, due to postgres version mismatch between lower(11.16) and prodution (10.2) environments, as older postgre version does not support exception handling. mi dataservices b team then made the code changes to mts nuget to force the shire calls to use only transactional batches to correct the behavior from the backend. releaseengineering team deployed the fix in production afer successful validation in lower environment, to mitigate the impact.(ecr chg0298685). company profile data is flowing and the issue is resolved.\n\n\n\nproblem\n\nerrors in ril workflow \n\nsolution\n\nindex key profile got changed from when validating the profile with the source, the code is considering the latest index key instead of the current day index key. dml was executed in prod staging to address this scenario and new transaction was triggered in staging.\n\n\n\nproblem\n\nunable to ping or remote into any production machines in ewdc \n\nsolution\n\nteam to upgrade the software of the switches on development and the production environments.\n\n\n\nproblem\n\nimpact assessment: sfdc email to case (e2c) delays--spdji \n\nsolution\n\nsalesforce s 3rd party vendor made a software update that caused emails to bounce. once the update was rolled back, emails resumed sending\n\n\n\nproblem\n\nunable to create new files on drive d on dmzprocess4 due to volume d had readonly attribute set to yes. \n\nsolution\n\nno issues reported after last incident, this was due to backup set drive to read only\n\n\n\nproblem\n\ncriteria workflow reviews application is not accessible via ratings gateway \n\nsolution\n\nthis issue occurred after the change chg0057015 was implemented for patches deployed for vulnerabilities and exploits as part of a vulnerable remediation project, post patching one of the web server was not started as criteria", "doc_id": "4667514a-1b33-42e0-b181-bdba9a4eadd4", "embedding": null, "doc_hash": "859b0a0164198c8166460e6bb0d3562a14194fc4435de6a0c2782209e5ec4f5e", "extra_info": null, "node_info": {"start": 58596, "end": 62644, "_node_type": "1"}, "relationships": {"1": "4632de4d-db37-491c-b70d-3e7740dfdae5", "2": "95f6aaa7-1b35-4200-b3de-75b233d37679", "3": "c6e49391-7565-46ae-9529-647c2b25c634"}}, "__type__": "1"}, "c6e49391-7565-46ae-9529-647c2b25c634": {"__data__": {"text": "remediation project, post patching one of the web server was not started as criteria workflow application were not aware this application has 2 apache instance and performed the post validation test only on the non sso url and had not verified on the sso url testing due to which users using the sso url were facing the error on accessing the criteria workflow reviews application\n\n\n\nproblem\n\nproduction: critical: col10dbsqlprd failed over to hycoldbpd01 from hycoldbpd02 \n\nsolution\n\ninstalled microsoft visual studio tolls for applications and configured proxy settings on hycoldbpd02. after these fixes, tested them by failing over the instance to hycoldbpd02 and confirmed that the jobs are running fine.\n\n\n\nproblem\n\nno price updates on rdf after receiving a record dropped message from reuters \n\nsolution\n\nwombat team implemented a code fix for this issue.\n\n\n\nproblem\n\nir websites and mi platform were facing intermittent issues \n\nsolution\n\nroot cause code: configuration error as per the case 119021119653087 raised with vendor microsoft, the team had evaluated the cause of spinlock contention on database servers - dmzsql1 dmzsql3 as slow tempdb objects creation which was impacting the performance of olp db. additional information: to mitigate the issue, vendor microsoft had suggested to enable the trace log 8005 and also suggested for modifications of stored procedures which were using tempdb heavily. post which, as a permanent fix, vendor microsoft had shared the on-demand hotfix to reduce the spinlock contention.\n\n\n\nproblem\n\nsan storage issue impacted limited functionalities on the ciq platform \n\nsolution\n\nimplement bug fix on storage arrays by upgrading version from mu4 to mu6.\n\n\n\nproblem\n\nlatency issues with running certain functionality for deal analyzer \n\nsolution\n\nthe underlying issue was the way storage was configured post aws migration of the application. there were still dependencies on the legacy nas system. once the storage subsystem was updated to use aws technology, performance was vastly improved\n\n\n\nproblem\n\nrca - inc0799484 - p2 - connectivity to secure ftp server(sftp.platts.com) was down \n\nsolution\n\nthe outage is due to cpu utilization is high\n\n\n\nproblem\n\nsso is down globally for external products, mi platform and capital iq \n\nsolution\n\nas per vendor f5 product development id 490174, need to update the load balancer os.\n\n\n\nproblem\n\nxml documents not rendering in mi platform \n\nsolution\n\nthe style sheet was reverted in our amazon s3 repository around 10:00 am. however, any user that opened a document and leveraged the improper style sheet would continue to encounter an error until cache was cleared from the udr buckets. therefore, the cache was manually cleared, and functionality was restored for all xml files\n\n\n\nproblem\n\nmajor incident - market intelligence - conning client from us-dallas location experiencing authentication errors while accessing snp global and ciq platforms. \n\nsolution\n\nas part of the vendor verizon case 2020031035758, verizon added statement (network 204.148.83.44 0.0.0.3 area 0) in ospf process on router eqr5 (mchill-eqr5-20525789e001) which was missed by vendor verizon during their change implementation for the 1 to 10 gig upgrades.\n\n\n\nproblem\n\nmonitoring alert reported slow response times on in ratingsdirect on ciq - inc1498796 \n\nsolution\n\nincreased the heap size and ram on the server\n\n\n\nproblem\n\nsimplify ccst circular reference error (case id: c26367) \n\nsolution\n\nmodel was updated and since the update there are no longer circular reference errors presenting with this model.\n\n\n\nproblem\n\nmajor incident - market intelligence - automated health checks of spgp (on-prem): all services/all users - are failing for random locations \n\nsolution\n\nas per defined process, asked noc to restart spark corosync services and then guided them to restart the spark servers which fixed this issue.\n\n\n\nproblem\n\nnot able to log into dotcms for vault", "doc_id": "c6e49391-7565-46ae-9529-647c2b25c634", "embedding": null, "doc_hash": "21b3d2c77af701b519b120791f55991a4d76bc8e734e688ae29c8db79232ebc0", "extra_info": null, "node_info": {"start": 62637, "end": 66564, "_node_type": "1"}, "relationships": {"1": "4632de4d-db37-491c-b70d-3e7740dfdae5", "2": "4667514a-1b33-42e0-b181-bdba9a4eadd4", "3": "3fe4e6a5-4bc2-4840-859a-4846053136fc"}}, "__type__": "1"}, "3fe4e6a5-4bc2-4840-859a-4846053136fc": {"__data__": {"text": "fixed this issue.\n\n\n\nproblem\n\nnot able to log into dotcms for vault \n\nsolution\n\nrestarted tomcats in rolling fashion.issue was resolved.\n\n\n\nproblem\n\nincomplete proforma report generation \n\nsolution\n\nadded a report validation job before reports are triggered\n\n\n\nproblem\n\nwfm analyst location change from dallas to farmers branch \n\nsolution\n\nupdated with changes to include farmers branch location\n\n\n\nproblem\n\nmajor incident - market intelligence - unable to access/login to multiple ciq production servers \n\nsolution\n\nunix servers were configured qts-dev-dc1.ciqdev.com and this server was rebuilt with new os and new name, nj15dc003.ciq.dev.com. unix support team replaced the old dc name with the new one to resolve the issues however as a permanent fix ad team has deleted the ciqdev.com dns stub zone from qts-inc-dc1.capiqinc.com and recreatedit. support teams confirmed that they were able to login to capiqinc.com unix machines with ciqdev ids.\n\n\n\nproblem\n\nmajor incident - market intelligence - data is available in the ir console but its not visible on the irw page \n\nsolution\n\nfixed index optimization job to prevent future occurrences.\n\n\n\nproblem\n\nrca - inc1421688,inc1389851 - p2 - pea (platts excel add-in) and apis - streaming connections were unavailable to internal and external users and hence were unab \n\nsolution\n\npas team has restarted the kaazing services on node1 and node2 to resolve the issue.\n\n\n\nproblem\n\nrca - inc1355829 - roll over dates were missing for london gas oil data\n\nsolution\n\ndatabase team manually inserted the rollover data as a temporary fix\n\nproblem\n\ncts financial queries troubleshoot \n\nsolution\n\nmt trigger was changed the way it was working to resolve the issue.\n\n\n\nproblem\n\nre-post reg jurisdiction report \n\nsolution\n\nworkaround was done to manually upload the file to s3 bucket\n\n\n\nproblem\n\nthe exisitng national scale rating does not display while cascading the rating in simplify. \n\nsolution\n\nscripts are applied to fix the issue\n\n\n\nproblem\n\nissues with esp and linx following wan optimization \n\nsolution\n\nto resolve the issue wipro nw team have bypassed the traffic in the riverbed from the source any to the destination server ip 10.169.40.133, problem is co-related to wan evolution project\n\n\n\nproblem\n\nfacing issue while connecting password for eqi_fgr_client on iisp \n\nsolution\n\nfixed it by placing the single quotes around the existing password in connections-aws-preprod.properties in preprod environment.\n\n\n\nproblem\n\ncst is not capturing fy2019 audit numbers \n\nsolution\n\ndvl cache needs to be refreshed post the finmaster data load.\n\n\n\nproblem\n\nthe criteri and guidance alternative investment funds methodology was published yesterday, we are not able to find the guidance in to model repository. \n\nsolution\n\ncriteria search is updated to search for all criteria\n\n\n\nproblem\n\nw-341897 - showing automatically as cancelled \n\nsolution\n\nenhanced the system to not automatically cancel jobs after 6 months of creation as long as users are actively working on it. users will now receive multiple email notifications before an idle job is cancelled.\n\n\n\nproblem\n\nin informatica | webinfop_rep spice folder, most workflow are long running \n\nsolution\n\nwfs migrated to spark\n\n\n\nproblem\n\njob folders are getting linked to taxonomy in docshare \n\nsolution\n\nafter enabling all failsafe jobs issue got fixed\n\n\n\nproblem\n\ncompare_gdb_eod \n\nsolution\n\ndml was executed to resolve the issue permanently\n\n\n\nproblem\n\ntemplate upload is not allowing indices that it should be \n\nsolution\n\nissue has been resolved by executing a dml\n\n\n\nproblem\n\nnew report: all-time high and", "doc_id": "3fe4e6a5-4bc2-4840-859a-4846053136fc", "embedding": null, "doc_hash": "35009c74b55bccbcae9aed4e487bd09eeacdfd087e2a4d4440a8e92a91432cb9", "extra_info": null, "node_info": {"start": 66578, "end": 70180, "_node_type": "1"}, "relationships": {"1": "4632de4d-db37-491c-b70d-3e7740dfdae5", "2": "c6e49391-7565-46ae-9529-647c2b25c634", "3": "8a380bfe-8a4f-463b-a4ef-1daeed0d6984"}}, "__type__": "1"}, "8a380bfe-8a4f-463b-a4ef-1daeed0d6984": {"__data__": {"text": "by executing a dml\n\n\n\nproblem\n\nnew report: all-time high and low report \n\nsolution\n\nissue has been resolved by executing a dml-closing the ticket\n\nproblem\n\naod(analytics on demand) portal not working \n\nsolution\n\nthis issue was due to java vulnerability work that was being done to update the vulnerable versions of java in the environment. it was later resolved by restoring java to the server.\n\n\n\nproblem\n\nmajor incident-market intelligence - slowness in loading credit research and investment research pages on ciq platform. \n\nsolution\n\nadded 2 searcher nodes to ir search cluster\n\n\n\nproblem\n\nserver ashrtapiprd12.prod.mktint.global was not reachable \n\nsolution\n\nunix team had worked with mi dashboard team to get the ilo ip and brought up the server - ashrtapiprd12, post which the issue was resolved.\n\n\n\nproblem\n\nxpertdoc db issue \n\nsolution\n\nthe transaction log has been set to auto resize and alerting is no monitored via two tasks\n\n\n\nproblem\n\naar is not working - error \n\nsolution\n\none of the web server was down we have brought up and fixed the issue.\n\n\n\nproblem\n\ncannot upload documents to box \n\nsolution\n\nthe box vendor has implemented the permanent fix to the issue.\n\n\n\nproblem\n\nmajor incident - all divisions - ticket creation not working in snow. \n\nsolution\n\naccess control restriction implemented\n\n\n\nproblem\n\nam users are unable to launch the look back review in production after pega822 upgrade. \n\nsolution\n\nissue got resolved by resaving the rule of data-portal.showdesktop to data-portal.showdesktop_lb and now either of them will work and should not be problem anymore.\n\n\n\nproblem\n\nthe workflow flow id is showing as undefined in wfm screen \n\nsolution\n\nfixed the code to verify that job id value sent all times.\n\n\n\nproblem\n\neasids being excluded when they should not \n\nsolution\n\nfixed include/exclude issue. when one of the asids is excluded (having common easids with other asid included)easid is touched and move to final ratings and recalcualte, the easids at that point are excluded from the job itself.\n\n\n\nproblem\n\nreport generation not starting after index calculation \n\nsolution\n\nreport thread count was increased to fix this issue.\n\n\n\nproblem\n\nspt - surveillance and publishing tracker - production \n\nsolution\n\nadded proper null checks for full review date objects in drools code.\n\n\n\nproblem\n\nno stream display errors and slowness issues in simplify - post patching. \n\nsolution\n\nthe mdc settings were updated in 12c for this issue.\n\n\n\nproblem\n\ndsos north america universe (universe_id=442 ) index calculation issue \n\nsolution\n\nindex setup is fixed and issue is resolved. closing this problem ticket.\n\n\n\nproblem\n\nlinx performance issues from dallas, boston, san francisco, chicago location \n\nsolution\n\nresolved the issue by moving configuration to local machine\n\n\n\nproblem\n\ni am unable to access the pending document basket. every time i click into pending document the system jammed and i cannot click into any job at all. \n\nsolution\n\nfixed loading/performance issue on the pending docs workbasket for sf and cgs by adding pagination, each page of size 200 records. users will see improved loading time.", "doc_id": "8a380bfe-8a4f-463b-a4ef-1daeed0d6984", "embedding": null, "doc_hash": "692d389e11aeb859899890e6da0c9231ee46b3f1acc15ef4d9e136abb33e937d", "extra_info": null, "node_info": {"start": 70178, "end": 73307, "_node_type": "1"}, "relationships": {"1": "4632de4d-db37-491c-b70d-3e7740dfdae5", "2": "3fe4e6a5-4bc2-4840-859a-4846053136fc"}}, "__type__": "1"}, "68ae190f-7e02-495e-83ea-3b834147d256": {"__data__": {"text": "Problem: Content users were experiencing slowness in SNL Document form and SNL DocViewer in \nWestX.  \nSolution:  Multiple support teams (DSE, Documents and Filling B, VMO, Storage &amp; Network \nSupport) were engaged to investigate the issue. The DSE team identified that there was a blockage \non the Bizwire loader?(D: \\Bizwire \\BizWireLoad.exe) on the SNLDB2 s erver. Documents&amp;Filling \nB team stopped Bizwire services on SNL Process (1, 7 &amp; 16) and then re -enabled them. \nDocuments &amp; Filling B team has removed the old 30K and 0KB documents from the input folder \nand also reduced the threading from 15 to 1 0 per server to mitigate the issue at 3:44 ET, \n05/09/2023.   \n \nProblem: Multiple Content Applications (CTS, West, WestX, Extractor, DocViewer, Celsus, Pathfinder) \nwere impacted. GCC application was also impacted.  \nSolution:  It was reported that Multiple Co ntent Applications (CTS, West, WestX, Extractor, \nDocViewer, Celsus, Pathfinder) across multiple locations (AMD, HYD, Manila and GGN) were \nimpacted. Network Data Team confirmed that there was a network interruption with Zayo 10G \ncircuit between CHO and ASH Data Centre which resulted in &#34; routing hang state &#34; , which \nled to network interruption for the traffic passing between both Data Centers . This is a known risk \n(R127) which is already logged. Network Data team removed and re -added the static rout e to \nmitigate the issue at 3 AM ET,5/8/2023.   \n \nProblem: Users are experiencing issues with ZScaler ZPA.  \nSolution:  IAM Ops team performed a configuration change to modify ZPA birthright Access rule for \nRatings Users in North America (RITM1213184). This c onfig changes disabled end -users SailPoint \naccess, causing the issue. The Access Rule change was reverted by the IAM Ops team at 09:36 ET. All \nthe impacted users are added back to the required AD group and the AD group is performing a force \nsync of profile s (ETA ~1 hour). All the impacted users are advised to use CISCO VPN through their \nEmail ID and Authenticator App as a workaround. Zscaler team has confirmed that after the Sync of \nAzure AD to ZPA SCIM, 4832 IDs are now available and will not have any issu es and the issue is \nmitigated at 00:30 AM ET, 04/29/2023. However, there are 18 missing IDs from all divisions, which is \na negligible count and can be ignored for now. The Ratings IM team was able to perform the initial \nvalidations and confirmed that 8 Rat ings users were able to access ZPA without any issues. Complete \nvalidations will be performed on 05/01/2023. A reconvene call has been scheduled. for 08:00 ET, \n05/01/2023 to discuss further course of action.   \n \nProblem: Priority Score and Sort Order was no t getting assigned in CT admin application due to \nfailure of a job attributed to a wrong value in the crawler admin page.  \nSolution:  It was identified by Data collection G team that the job \n(job_AssignDocumentPriorityRank_prc) on FDCA server is failing at  4th step from 05:06 AM ET \nonwards and is throwing the error: Arithmetic overflow error for data type tinyint. It was also \nidentified bad element values collected against type id 510 for couple documents (1930490316, \n1930490450) in the tables (DocumentElem entInteger_tbl/ ", "doc_id": "68ae190f-7e02-495e-83ea-3b834147d256", "embedding": null, "doc_hash": "c5e5ed57d8a12462a0dd55a5cb800e18b374776af07897dd7932880e37871d15", "extra_info": {"page_label": "1", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 3244, "_node_type": "1"}, "relationships": {"1": "87e59040-0e3d-4d29-ac73-6666cb8c4440"}}, "__type__": "1"}, "eb674d8b-2aec-4437-8d81-888b95da3b96": {"__data__": {"text": "DocumentToObjectRelElementInteger_tbl). The same are deleted by Data collection J team from \ncouple tables to resolve the issue at 14:35 ET. The job is running successfully now.   \n \nProblem: Extreme Slowness in Content Collection Applications  (CTS/WEST/WETX/Extractor and \nDocviewer) due to a MT Schema Server issue  \nSolution:  The MT Support team observed errors on one of the Schema servers \n(CHOSCHEMAPROD01) which seems to be the cause of issue. MT Support restarted the ?Schema \nservice to resolv e the issue at 6:43 AM ET. The Content teams confirmed that the content \napplications are working fine and they are no longer experiencing any slowness.   \n \nProblem: Content Apps dependent on SNLDB2 & SNLDB3 were down due to a ISP Link (Fiberlight) \nflap issu e  \nSolution:  Multiple Support teams (Network Data, Network Security, Wintel, DSE, Content users) \nwere engaged to investigate the issue. The network data team has confirmed that there was an ISP \nLink (Fiberlight) flap which has caused the issue. The Impact ed ISP link (Fiberlight) has been shut \ndown at 10:25 ET, 05/02/2023 to mitigate the issue and the traffic is traversing through the \nsecondary ISP Link (Zayo). Content users have validated and confirmed that they are no longer \nexperiencing any issues.   \n \nProblem: Multiple users receiving errors when opening stories in CCP  \nSolution:  TechOps restarted services on the prod -us-chp1 -us-ccp node.   \n \nProblem: Stale Consensus Estimates data on CIQ  & CIQ Pro due to an Informatica issue  \nSolution:  \" It was identif ied that intermittent issues were observed between Informatica and \nConsensus Data refresh service since 4/26/2023, due to which queue built up. Although, queue \ncaught up on 4/27, but it was observed that Consensus Estimates Data was reprocessing older \nmess ages from Informatica and causing bad data. A case with Vendor (Informatica # 04482901) has \nbeen logged to identify the cause of the issue and investigate further. CIQ Dev team identified the \njobs which were not processed from the backlogs. IM -CSD-Estimate s Standardization disabled \nincremental flow to Consensus Calculator app, purged the existing queues, post which Estimates \nCalculator was brought to process all backlogs. The estimates team completed processing the \nbacklogs for CIQ (1.2 million) and CIQ pro  (3.5 million) at 7:08 AM ET, 05/02/2023 to resolve the \nissue.  After restarting Informatic UME store by SOA team issue got resolved.\"  \n \nProblem: Users working from Manila & Pune ODC (SPGI & IHS) were unable to access internet via \nLAN due to a power failur e on a legacy switch in Secaucus DC  \nSolution:  The onsite Engineer visited the Datacenter and powered on the switch which mitigated \nthe issue.   \n ", "doc_id": "eb674d8b-2aec-4437-8d81-888b95da3b96", "embedding": null, "doc_hash": "20262f7faaf6c0b6ca8391ffbed7201c65e06790a66c4d0b6c1dfa5e10fed64b", "extra_info": {"page_label": "2", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 2741, "_node_type": "1"}, "relationships": {"1": "652fe5bc-5b76-46c9-a3af-d2cd5d0b5b8e"}}, "__type__": "1"}, "b38c771b-9a24-4fb0-bfd5-bf1ef86b5e97": {"__data__": {"text": "Problem: IndexMetadataFeed - Error occurred when validating remote object \nGRIP.INDEX_DEST_FOR_IDS@GRIPXP  \nSolution:  DV team have executed the ARPING command for the Nutley DNS   \n \nProblem: Documents in EDCA application were picked up in delay due to  timeout error on the SQL \nDatabase Server   \nSolution:  1   Multiple teams (MI Database, Scrum Mount Trisul, Content SRE) teams were \nengaged on bridge call to investigate the issue. Scrum  Mount Trisul team identified that there was \ntimeout error on the SQL Database Server (COL21DBSQLPRD \\COL21),? and indexing job running slow \nwhich had caused the issue. MI Database team have killed the Indexing job to mitigate the impact \nat? 16:42 ET, 04/28 /2023. Validation has been performed by the Users which states that \nAutoExtractionService are now picking up the documents.   \n \nProblem: External clients & IHS users were experiencing intermittent issues with email delays due to \nan exchange server connectio n to AD issue  \nSolution:  Messaging team has confirmed that they identified two problematic Exchange servers \n(VA06EXM5005 &amp; VA06EXM5006) that have caused the intermittent issues and the email delay. \nMessaging team removed impacted servers from the clus ter at 10:30 ET, 04/28/2023 to mitigate the \nissue. The remaining servers (NJ12EXM5005 &amp; NJ12EXM5006) have processed the backlog \nmessages and the backlogs cleared at 11:30 ET, 04/28/2023. No pending emails in the queue.   \n \nProblem: Documents were not fl owing to downstream applications from Foreseer Application due to \nfew filings mistakenly added to production instead of Staging  \nSolution:  Extraction A team, identified that the issue was caused due to few filings that were \nmistakenly added to production by Fundamentals content team instead of Staging CT Admin. Due to \nmultiple retry attempts for these bad filings, the queue built up and delay was observed in \nprocessing the documents. Extraction A team stopped the NiFi processes to stop the flow of new \nfilings and cleared the bad filings from queue to resolve the issue. The queue cleared at 10:28 AM ET \nand the NiFi process was restarted.   \n \nProblem: Publications are not making it to Alphamail.  \nSolution:  Missing reports were manually re -published and DBOps cleared the DB blocking sessions \nafter which the remaining publications that had been stuck processed automatically.   \n \nProblem: Delay in delivery of profiling data from CIQ to CIQPro Platform due to changes rolled out in \nproduction against a user story  \nSolution:  F1 Avengers team identified that the issue was caused due to changes rolled out in \nproduction against user story (5900984) on 4/25/2023. The Data Architecture team rolled back the \nchange to fix the issue. However, App pool recycle which was sched uled at 6 AM ET did not happen. ", "doc_id": "b38c771b-9a24-4fb0-bfd5-bf1ef86b5e97", "embedding": null, "doc_hash": "6e3e5337238a1f4ac1c1bb1adc393fa52dd455cc2abf0aaa701833a203d8b6d3", "extra_info": {"page_label": "3", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 2818, "_node_type": "1"}, "relationships": {"1": "0c4b39a6-7503-4ca9-9446-3a83ae2eed34"}}, "__type__": "1"}, "87d63a19-01f5-4fba-ad33-28a944c52ad4": {"__data__": {"text": "Hence, MI Data Services B team manually recycled the app pool for the changes to come into effect , \nthereby mitigated the impact at?7:30 ET,4/26/2023 post which backlog data started flowing and \nwhich got cleared to resolve t he issue at 12:09 ET, 4/26/2023 .   \n \nProblem: Internal/External users are not able to search the forward curve symbols under Forward \nCurve Charting under Platts Dimensions Pro  \nSolution:  The CloudOps team restarted linkerd and microservice deployments tha t injected linkerd \nproxy containers into the pods which helped in service -to-service communication &amp; resolved \nthe issue.   \n \nProblem: Delay with RatingsXpress V4 SPAN Files due to a job failure   \nSolution:  Ratings Xpress feed team identified that the a utosys job \n(p_mi_xf_PullMerge_RX_CoreInstrumentRevenueSource) failed on SQL server \n(RXF01DBSQLPRD \\RXF) as one of the record (CHARTER SCHOOL) ?in SectorID inactivated caused this \nissue. PSS team ran the autosys job to mitigate the impact at 1:13 ET,4/25/202 3.   \n \nProblem: Issue with US Tagging Server as Elastic Search Service was consuming High CPU Utilization.  \nSolution:  TechOps increased JAVA space memory on the US Tagging Server from 1GB to 6GB \n(emergency CR?CHG0326576).   \n \nProblem: The SPGLOBAL site that  hosts Crown peak pages was not accessible.  \nSolution:  The SPGLOBAL.com site that hosts the Crownpeak pages are not accessible and multiple \nTechnical Teams tested the application servers and refreshed the load balancers, but the issue \npersisted.Crownpeak (Vendor) has been engaged with the case #83382. The Crownpeak vendor \nteam verified that inbound traffic to the site was stable throughout the day and did not see any \nissues. In addition, the CrownPeak Vendor Team have performed multiple IIS (Internet Infor mation \nServices) restarts, but the issue continues to persist.he Crownpeak support team was able to mount \nthe backups and reconfigure Microsoft IIS to server to reference the backup disks instead of waiting \nfor the completion of full copy/restore to the se rver disk. The site came back up at 20:10 \nET,04/20/2023 after restarting IIS and has been stable without any site interruptions in the EU -WEST \nPOD. The US -East POD was also brought into the rotation, with the same mounted drive and Ratings \nsite pages were validated and working as expected after mounting to the drive. This configuration \nwill remain in place until the restore of content to the actual server drives and there will be a \ncontent publishing freeze to the sites until full restoration is completed o n 04/21/2023 .Crownpeak \nVendor team is going to built a script to incrementally run though the published items from the last \ntwo days in small batches while monitoring the performance of the site.Teams confirmed that \npublishing scripts has been completed a nd they haven&#39;t received any further alerts from the \nsite. The site is stable and fully back on the original disks with updated content. Initial validation of \nall content seems to be functioning as expected. Teams to conduct further in depth testing on  \n04/24/2023 to ensure all other content sourced from automated processes has been accounted.   \n ", "doc_id": "87d63a19-01f5-4fba-ad33-28a944c52ad4", "embedding": null, "doc_hash": "3f0c381451d35b34ce7b6ae308eaddcbd1402e402a5ae50687befc39f8b6ff95", "extra_info": {"page_label": "4", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 3190, "_node_type": "1"}, "relationships": {"1": "5eed7f71-92c7-47b1-ac9d-50e9548dd537"}}, "__type__": "1"}, "ab477405-457d-421d-a3ad-15467c5a9762": {"__data__": {"text": "Problem: Ratings users were unable to save/upload files in Box.  \nSolution:  Box support team had raised a case with the Vendor (Box case #2836213) Network \nSecurity raised a Zscalar Sev1Case# 04076495. Network Security team has bypassed all BOX URLs \nfrom proxy and added a firewall rule with port 443 to mitigate the impac t for few locations (Japan, \nAustralia, Hongkong, Singapore, Sydney &amp; Melbourne). (Emergency Change - CHG0324448). \nFew users were still facing issue. BOX vendor team Confirmed that IP Address (107.152.29.224) was \ndecommissioned and should not be in use anymore and suspect that this may be the cause. Network \nSecurity team has raised an emergency change (CHG0325030) to Disable Invalid IP from BOX for few \nof the APAC Locations (to check if impact gets mitigated. As per Box Vendor recommendation the IP \nis a part of /20 prefix, the Emergency change (CHG0325030) implemented by N/S team to Disable \nInvalid IP from BOX on 04/18/2023 has been reverted (107.152.29.0/24 is re -enabled). BOX Vendor \nhas confirmed that they had stopped serving the content / CDN traffic s ince last year AUG 2022. \nNetwork Data &amp; Network Security teams identified that the AD servers are forwarding requests \nfrom server (upload.app.box.com) to Singapore Infoblox DNS forwarder (SG07ibx105.mhf.mhc \nInfoblox) and it is returning to decommission ed BOX IP (107.152.29.224).? Network Data team raised \na case with vendor (InfoBlox case#00388287).? Box vendor team has confirmed that they have \nidentified NS1 was returning incorrect DNS in some cases and hence they deployed a fix to resolve \nthe issue at 16:05 ET, 04/20/2023Teams validated with users from all the APAC region who are \ncurrently working from office and got the confirmation that they are able to save/upload file on box.   \n \nProblem: Content users were getting error message as \" Exception in clo sing the Job \" while \ncommitting documents in Foreseer application due to server application pool being  stuck/flooded   \nSolution:  Multiple teams (DSE team, Wintel Team, Extraction A team) joined the call to investigate \nthe issue. DSE team checked and conf irmed that there is no issue from database side. Extraction A \nTeam suspected that there was an issue with Server Application pool (II48ISP5001/II48ISP5002) \nwhich got flooded. MI Content SRE team recycled the app pool at 2:50 AM to resolve the issue. \nUsers have validated and confirmed the issue is resolved.   \n \nProblem: Intermittent access issues on Capital IQ plugin on CIQ Classic platform due to tempdb \ncontention on the OSLO SQL Database server  \nSolution:  Multiple support teams (DSE, CIQ SRE, and Storage) was engaged on the call to investigate \nthe issue. CIQ SRE team recycled the app pool on the web servers - (qtciqwbpd01 -08). DSE team \nidentified a tempdb contention on the OSLO SQL database server.? DSE team restarted the OSLO \nSQL Server to mitigate the iss ue at 13:35 et, 04/21/2023. Validations have been performed and \nconfirmed that the issue has been mitigated.   \n \nProblem: Few placeholders are randomly copying RAP links in case of Create placeholder(Delete \nplaceholder already handled)  \nSolution:  we have a pplied a fix to rebuild workspace placeholder list afresh from database \nwhenever a placeholder is added. Same fix was applied for placeholder deletion as part of March 10 \nth release.   \n ", "doc_id": "ab477405-457d-421d-a3ad-15467c5a9762", "embedding": null, "doc_hash": "422f419325520a270b0d1368314347321acfc484fb72dca9f7b43957f49d6945", "extra_info": {"page_label": "5", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 3373, "_node_type": "1"}, "relationships": {"1": "bcc7d2ad-e32c-4171-874c-bf2d5fe69c5f"}}, "__type__": "1"}, "ba6c56bb-536f-49e4-b57a-ef05088e22bc": {"__data__": {"text": "Problem: Span files were delayed on RatingsXpress V4 due to SQL databa se server constraints.  \nSolution:  It was identified that Span files was delayed on Ratings Xpress V4 since 15:44 ET, Rattings \nXpress team identified that SQL Database Server (RXF01DBSQLPRD \\RXF) had two strings (Louisiana, \nLA Appropriation Service Contract  P3 project) and (Louisiana, LA Appropriation Service Contract P3 \nProject) that two strings acted as a same string which caused this issue. Ratings Xpress Feed team \nwas restarted the database job (p_mi_xf_PullMerge_RX_CoreInstrumentRevenueSource) on the SQ L \nServer which issue got mitigated at 17:56 ET, 04/17/2023. No further clients cases has been reported \nso far since 15:44 ET, 04/17/2023. Validations have been successfully completed with the user and \nconfirmed that the issue has been resolved.   \n \nProblem:  Multiple Functionalities in CIQ Pro platform experienced degradation on US -EAST stacks \ndue to huge influx of calls from Portfolio analytics via Data API Services saturating the system.  \nSolution:  It was reported that multiple critical functionalities (Co mpany Profiles Pages and \nDocViewer) experienced performance degradation on US -East (Stack 1 and 2) . CIQ SRE team moved \ntraffic from US East to US -West to mitigate the impact at 7:40 ET, 4/17/2023. Upon further \ninvestigation , CIQ SRE Team along with Deskt op Architecture team identified that Portfolio Analytics \nmade large number of calls via Data API Services, which overwhelmed the system caused this issue. \nPerformance QA Team performed load test on US East Stack 2 .Since, Services seemed stable, CIQ \nSRE Te am put back US East into the mix to resolve the issue at 10:21 ET, 4/17/2023.   \n \nProblem: Content users experienced slowness while performing activities in Compustat data \ncollection application (FRDE) due to high CPU utilization on few of the Citrix prod s ervers  \nSolution:  The Sybase processes were killed, and the impacted Citrix servers were restarted, however \nthere was no improvement. Wintel support has doubled the capacity of the virtual CPUs for the \nCitrix servers in production to mitigate the issue.   \n \nProblem: \" AutoSys - # HIGH # Status:  Job: <p_cfs_api_entitlements  \"  \nSolution:  MC_OTC_INT_CFSDS_UPD_DATALOAD_STATUS was killed and the Informatica DB team \ntriggered two jobs to close out the current record in the cfs_stg_dataload_status table &amp; al low \nsubsequent CFS loads and feed files to run.   \n \nProblem: Blue Prism automation Bots were down due to connectivity loss between Azure & On -\nPrem vs load balancer.  \nSolution:  Network Security team confirmed there was connectivity loss between Azure &amp; On-\nPrem Virtual machines? which resulted in Blue Prism automation Bots going down? As a part of \nChange (CHG0315835 -Firewall upgrade) ,after rebooting the firewall ,BGP session was cleared and \nswitch took other path? SNLDMVPN on firewall (II48IRT015) Networ k Data team ?modified the route \nconfiguration on the core switch (II48CD101) and diverted traffic to firewall II48TFW ?to mitigate the \nimpact at 5:10 ET, 4/10/2023.   \n ", "doc_id": "ba6c56bb-536f-49e4-b57a-ef05088e22bc", "embedding": null, "doc_hash": "0bca31e1ec33090d68a34523e2f6d3f39b0038769c69fc22486353d9f1272670", "extra_info": {"page_label": "6", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 3110, "_node_type": "1"}, "relationships": {"1": "768622ef-31a3-49dd-8de4-0043010c7bc9"}}, "__type__": "1"}, "32368d5d-c962-4fbc-99c5-f5b923d1957f": {"__data__": {"text": "Problem: Addiing Team Members to RELEASE TEAM / Post Release Workbasket Issue  \nSolution:   1) RELEASE TEAM -?Added missing records in work group table for this team.( Table Name: \nSIMPLIFYDATA74.pr_data_admin ), this records insert from Pega Designer Studio Class instances. 2) \nPost Release Workbasket -?Added missing records in Region Hierarchy &amp; Business Hierarchy \ntable.( Table Names: SIMPLIFYDATA74.SP_TEAM_REGION_HIERARCHY &amp; \nSIMPLIFYDATA74.SP_TEAM_BUSINESS_HIERARCHY )   \n \nProblem: Issues with pipeline during DB patching exercise that caused East DB Patching to be put on \nhold  \nSolution:  \" Spcom failover ansible code failed as the dependent aws repo branch it was referring to \nwas missing in github. SP -RATINGS -Ansible -aws rep o was using dmz -spcom -redesign -no-proxy -\nfailover which got deleted during branch cleanup. Issue is fixed with new branch creation and \nupdated changes.   New branch is tested on alpha failover is working fine.? \"  \n \nProblem: Unable to create Hierarchy Review  case when  length of Entity Name  Exceeds 64 chars and \nspecial characters.  \nSolution:  Due to field length discrepancy error was triggered. field length is increased and made \nconsistent   \n \nProblem: Filings accumulating in Extraction Service  on Foreseer A pplication due to requests not \ngetting pushed to active Messaging Queue.   \nSolution:  Fundamentals team reported that they are seeing Filings Accumulating in Extraction \nService on Foreseer Application. IM -DT-Fundamentals -Transformation -Dev confirmed that r equests \nwere not getting pushed to Active MQ (Active Messaging Queue). DT foreseer team restarted the \nActive MQ at 12:00 AM ET to mitigate the impact and new Filings started processing. IM -DT-\nFundamental -Transformation -Dev team completed the reprocessing o f the Filings previously \naccumulated before the fix was applied. Validations are successful.   \n \nProblem: Rating Advisor RRO submission created a Lead not an Opportunity  \nSolution:  Now we are sending the user selected entity in case of ratings advisory whe n user search \nany rated issuer in the search box and storing in session storage?   \n \nProblem: Akkurat LL Fonts are missing in CAP windows servers  \nSolution:  We suspect this font installation done through sharepath and it deleted after server \nrestart. Howev er We have redeployed fonts again in production servers as part of April release and \nmonitoring also in place hence resolving this.   \n \nProblem: Annual Review CRA3 article publications  ", "doc_id": "32368d5d-c962-4fbc-99c5-f5b923d1957f", "embedding": null, "doc_hash": "27dc77131dede4c35816f9f9dd7f8982d2a2ffc4f7aca81b0f134a9d0a8929c1", "extra_info": {"page_label": "7", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 2518, "_node_type": "1"}, "relationships": {"1": "691a64d4-db7c-4b2f-911c-e6e6223326d0"}}, "__type__": "1"}, "508721b8-cb18-4115-87ac-28d5b284d4ba": {"__data__": {"text": "Solution:  \" We have removed the where condition check to related to CR A3 at the denodo level and \nfetching the entire data and then written the logic to filter out the instrument related to CRA3 \nentity. So it will remove both parent and child entities related to that CRA3.   Fix is deployed to QA \nand UAT environment. Fix is t ested and certified by QA . We have done UAT with users and?got the \nsignoff.   \n \nProblem: Remove Black JPCR from PCR list on  S&P.com  \nSolution:  This issue will be handled as part of the JPCR Republishing Feature Project (FEATURE \n5814962). We will ensure t hat when an entity in Japan has both a Financial Enhancement Rating \n(FER) and an Issuer Credit Rating (ICR) with a JPCR (JR indicator &#61; ?Y?), the system identifies the \nFER as an entity -level rating, and that?when a Financial Enhancement Rating (FER) is  processed, the \nsystem does not consider this as an Issue Rating.   \n \nProblem: Variable section data missing in DA  \nSolution:  DSP service should be migrated to KONG   \n \nProblem: Upon deletion of older RA job, the new job created does not refresh when opened   \nSolution:  \" We have made the changes in code so as to get the updated status for the jobs.?Once \nthe old RA -job is cancelled and its status it set to cancelled, the updated status will now be?mapped \non the page instead of mapping the old status of the jo b.?   Called the Activity in step 1 of \nRuleset?SPRTG:18 -04-90.   Call RTG -Work.ValidateDuplicateOrgAndRole and comment off the step \n2.1.4 to avoid the double time property set. \"  \n \nProblem: As simplify is observing that there is change in the existing and recommended scores as we \nare fetching  different existing scores in the RAP & Simplify. Simplify system is blocking the user due \nto th  \nSolution:  \" Fixes done from RAP side:   # Updating Input and Output codes: ??? a) Need to update \nlabel and meLabel for business_position &amp; business_position in model -scenario -lovs.const.ts \nfile   # Summary Section: ??? a) Need to change the displaying logic for Business and Risk position \nvalues   # Business Position area: ??? a) Need to change the input mnemonic code a nd lov&#39;s \ndata point ??? b) Need to change the output mnemonic code ??? c) Need to change Highlighting the \nchange logic ??? d) Need to change the required for SACP Calculation logic ??? e) On change, need \nto? change update Model Engine value logic(in up dateWithModelEngineData method)   # Risk \nPosition area: ??? a) Need to change the input mnemonic code and lov&#39;s data point ??? b) \nNeed to change the output mnemonic code ??? c) Need to change Highlighting the change logic ??? \nd) Need to change the requ ired for SACP Calculation logic ??? e) On change, need to? change update \nModel Engine value logic(updateWithModelEngineData method)   # Model Engine Area: ?? a) Need \nto update the meLabel&#39;s? in model -scenario -lovs.const.ts to reflect the input values c hagnes ?? \nb) Need to chage the logic to update the model engine output values(in setModelEngineOutputData \nmethod) ?? c) Reset model output values, need to change the logic to reflect the changes(in \nresetModelEngineOutValuesOnUi method)    # List View: ?? a ) Need to update the mnemonic codes ", "doc_id": "508721b8-cb18-4115-87ac-28d5b284d4ba", "embedding": null, "doc_hash": "b2d55cc83ab9dfe2d0efbb7dcbd6a3427cd76b784322d77bb0decb80e206ee8b", "extra_info": {"page_label": "8", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 3238, "_node_type": "1"}, "relationships": {"1": "eafad47f-6db6-44d9-9513-3fbdabb3fd5c"}}, "__type__": "1"}, "dd23d8bf-ed1a-4fcc-8322-5e27d60843ca": {"__data__": {"text": "to reflect the new changes  # UI - Test cases: ?? a) Need to updated few test cases in model -\nscenario -sample.component.spec.ts to work with the new changes?  # Observations in Lov data(SI, \nQA &amp; UAT): ? for BUSINESS_P OSITION_ASSESSMENT, RISK_POSITION_ASSESSMENT, the Very \nStrong lov value coming without space in between       \n \nProblem: Search for Object cannot be performed when there are special characters in Entity.  \nSolution:  When calling Search object service pega will be passing escape character for the service to \nbe able to parse when there is a double quote character in t he search string   \n \nProblem: New entry to add list of organization that have recevied a RES or CA.  \nSolution:  As per SOP Ops team will update RES/CA Org invoice dates in RPM and Mongo DB.   \n \nProblem: RPM does not let me attach documents  \nSolution:  As par t of the fix, we are indicating the user to enter up to 64 characters only and ensuring \nthat user can only enter within the limit in the subject field available in the RL job attachment \npopup?   \n \nProblem: CDO Evaluator is Missing From the Models Page  \nSolu tion:  Archiving newly created test version, model is displayed on r360. \"  \n \nProblem: New Bicra template is not pulling the scores into RAMP (Enhancement CAP May 2023 \nRelease)  \nSolution:  Simplify will pass CAP A flag saying iti is Birca org, CAP will pass  that flag to Data service, \nData Service will reverse lookup the ID and get the data to come back in the service response.? Data \nwill then flow to the RAMP tables.   \n \nProblem: Log4j Vulnerability   - Impacting multiple DMZ and Internal Applications  \nSoluti on:  \" Team continue to address log4j vulnerabilities that have evolved since the initial \nidentification in December 2021.   We are taking steps to ensure that teams prioritize updates to \naddress these vulnerabilities through CAB, ADO Pipelines and excepti on logging.??   The remediation \nof additional log4j vulnerabilities will be moved into BAU support and there will likely be renewed \noffice hours around Vulnerability Support overall to express the need for teams to keep their log4j \nfiles updated.? There ar e exceptions in place for some apps that could not make the required \nchanges.? Those teams will need to ensure those exceptions are maintained and not expired and \nrenewed where necessary. \"  \n \nProblem: Enable ROS  Team  to  publish/Extract  JPCR  for ticket  reduction.  ", "doc_id": "dd23d8bf-ed1a-4fcc-8322-5e27d60843ca", "embedding": null, "doc_hash": "787cfec2be6611c5bbadaea45680f69f1a015646c29ae3e2cc81cdc592b4892e", "extra_info": {"page_label": "9", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 2439, "_node_type": "1"}, "relationships": {"1": "afa2c7ce-d93c-4acb-a3f2-9063ef2a043b"}}, "__type__": "1"}, "045d87d4-2d35-4794-8fcf-f64219687636": {"__data__": {"text": "Solution:  Build the functionality to provide Japan ROS the ability to perform the following: 1) Extract \nJPCR XML 2) Upload JPCR XML 3) View amended PCR in PCR Preview of RPM job 4) Republish JPCR \nwith amended XML   \n \nProblem: Multiple users we re unable to launch applications hosted on Citrix Workspace  \nSolution:  Multiple teams (Citrix, Network Data, Network Security, AWS Cloud) were engaged. \nNetwork data team found that the issue is not related to ZPA since the applications are internet \nbased and hosted on Citrix cloud. As part of troubleshooting, Network Data team disabled internet \nsecurity (ZIA proxy) option in Zscaler application (ZCC), post which the applications were launching. \nNetwork Security team raised a Sev # P1 Case (03796236) with Z scaler vendor to troubleshoot t \nproxy level. Wireshark logs were captured and shared with the vendor (Zscaler). Network Security \nteam added URL (spglobal.cloud.com) in Zscaler App connector and updated the policy for few \nusers, as a workaround recommended by vendor (Zscaler), but the issue persisted. Network Security \nteam has hitelisted few more URLs suggested by Citrix vendor, but the issue remained same. \nWireshark logs were again captured by Zscaler vendor for further anlaysis. Network security team \nthen bypassed added &#34;spglobal.cloud.com&#34; Zscaler portal windows app profile to mitigate \nthe issue at 11:15 EST, 12/06/2022.   \n \nProblem: Content users are unable to open company profiles in Celsus.  \nSolution:  DSE team identified blocking on database ser ver (QTCOL01DBSQLPD \\QTCOL01DBSQLPD) \ndue to a long running user query from server (QTMSCTSIN011). DSE team killed the user query to \nresolve the issue at 5:25 AM ET.   \n \nProblem: Auto article instantiation fails  \nSolution:  Updated Java certificate file by importing required server certificatie.   \n \nProblem: CIQ site was running extremely slow and while searching for companies or data set an \nerror message was received.  \nSolution:  The issue is resolved by restarting  the Zookeeper nodes. However the SOA team is \ninvestigating with the application team on improving the resource levels and configuration to \nprovide better stability and availability. The work has started with the current sprint 22.02.06 and \nwe will close t he open items asap.  \n \nProblem: Users reporting multiple issues with Pathfinder  \nSolution:  Log backups were not happening as the backup preference was set to secondary as a \nresult the log file got full on the primary database. Changed the backup preference  to &#39;Any \nReplica&#39; to take the log backups on the primary. We do not anticipate these issues again.   \n ", "doc_id": "045d87d4-2d35-4794-8fcf-f64219687636", "embedding": null, "doc_hash": "3acc16c816c1248678b3ac24e945db2b8901b31d63f927cb3cef64afceb56a0b", "extra_info": {"page_label": "10", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 2655, "_node_type": "1"}, "relationships": {"1": "61d5c535-efe6-4331-b2d2-52dda0ca0717"}}, "__type__": "1"}, "ebfd07ff-6b10-4068-8245-9567603012da": {"__data__": {"text": "Problem: Users are unable to access Docshare links which are share via email once the jobs is \nreleased  \nSolution:  We have replaced the functional ity by using insight protocol with chronicle id of the \ndocument. This will eliminate the dependency on the document location. Change is moved verified \nby QA and deployed in Prod on 03/31/2022.?   \n \nProblem: Security calculation issue for  S&P/ASX bank bill index   when rebalancing day falls on a \nholiday.  \nSolution:  this is fixed as part of release on 16april  CHG0233674?    \n \nProblem: Users are experiencing intermittent login issues on CIQPro  \nSolution:  Worked with EPS (End Point Security) Team and whitelis ted folders. Additional trace flags \nadded to mute dump generation while MS fixes bug in Product as part of SQL Server 2019 &#43; \nCU16.   \n \nProblem: Data Loader Service not running daily as per schedule automatically.   \nSolution:  CDC processor was running i n the streaming cluster. Now, its changed to JOB Cluster   \n \nProblem: Incorrect Industry Score populating for Switzerland  \nSolution:  Fix provided in prod. Industry Risk for Switzerland is hard coded to get correct value   \n \nProblem: User requested access to  APAC, EMEA and US templates  \nSolution:  This was a user access issue and the user was re -directed to the ARP Team who were able \nto resolve the issue.  \n \nProblem: 973491 - Not allowing ICR RL to be drafted; job cannot be sent to billing  \nSolution:  Enabled t he logs to identify the issue on next occurrence and logs are planned to move \nproduction.   \n \nProblem: Unable to move forward on Ratings Screen due to EDC.  \nSolution:  Made logical fix when EDC - when NA - Not Applicable option is selected for EDC, this NA \nvalue  tag  is not sent to Core at all, as the tag is not sent to Core it doesn't validate anything and \nallow case to process. Alpha names will bypass the v alidation stage. This is a permanent fix, \ndeployed via CHG0250571 on May 7  2022.   \n ", "doc_id": "ebfd07ff-6b10-4068-8245-9567603012da", "embedding": null, "doc_hash": "561b0c33e8c0cf3cf7a4315212dc84d570a4f2ad14ddcf8f63ad7ff5a437969e", "extra_info": {"page_label": "11", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 1965, "_node_type": "1"}, "relationships": {"1": "e346ceeb-1c15-42ad-84ba-fd5522608f29"}}, "__type__": "1"}, "e235e797-b7ff-4299-8e59-8dc2ebeb4921": {"__data__": {"text": "Problem: R360 - Missing performance data on UI -(Timing issue)  \nSolution:  The missed Id&#39;s have been reprocessed and verified in R360 UI. After reprocessing \nthe data i s available on R360 UI.   \n \nProblem: DR deployment was not 100% successful for rap -servingdeployerwin -caas  \nSolution:  \" Corrected the host name in the YAML file and added the missing key \nRAP_OAUTH_TOKEN_USERNAME         \n \nProblem: User transferred from MI t o Ratings and caused profile issues/Multiple Names in system  \nSolution:  There is now a feed from Sailpoint to ARP - there is additional work being undertaken to \nautomate tasks from the Sailpoint   \n \nProblem:  \"502 Bad Gateway\" error while creating data sou rce for the v_pw_performance view in \nImmuta  \nSolution:  The issue has been resolved after upgrading Immuta to 2021.5. Now, the data source is \nbeing created and the view can be queried from reporting cluster.   \n \nProblem: Unable to load documents in Simplify  case.  \nSolution:  \"  Fix:   as part of child requestor initiation process we are verifying the run time context \nand resetting the context if it is getting the wrong context. Context will reset to Simplify after RPM \nand WFM are used, instead of leaving it in the most recent state. If multiple applications (RPM, WFM \nand Simplify are opened at the same time, Simplify will be prioritized.  Attached the change to the \nPRB \"  \n \nProblem: Unable to execute Python Commands in Databricks Notebook resulting in Timeout Error  \nSolution:  Its Server Restart. No Permanent Fix needed  \n \nProblem: When signoff analyst tries to signoff an AR, takes them to RPM screen  \nSolution:  Post updating right endpoint URL cases are now routed to correct application SIMPLIFY   \n \nProblem: Mul tiple Content Applications were not working as expected on both Local and Remote \nApps. The Snltxprocess memory leak issue.  \nSolution:  Completed - Replaced that code with a method using native C&#43;&#43; and the memory \nleak when away. This was rolled out t o production2/1/2022.   \n ", "doc_id": "e235e797-b7ff-4299-8e59-8dc2ebeb4921", "embedding": null, "doc_hash": "ea477bf7924e6e10a8f2de8bf85fb7caf7780d6a472f3b33617e3d84aac83cfd", "extra_info": {"page_label": "12", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 2031, "_node_type": "1"}, "relationships": {"1": "116c8dbf-037a-4200-be81-e5c0319e9698"}}, "__type__": "1"}, "e2f9288d-599c-4971-9602-b98426ce1ded": {"__data__": {"text": "Problem: Excel files in Docshare are opening only in Read -only format.   \nSolution:  Excel files read only issue was fixed by updating the configuration files in Docshare servers \nwith Emergency CR #CHG0219778   \n \nProblem: Incorrect Date showing up in the Reports Section  \nSolution:  Updated date format provided through code fix.   \n \nProblem: Intermittent login issues on Cflow  \nSolution:  Technical teams restarted the CFlow Webgate servers in a rolling fashion followi ng which \nusers confirmed they could log in to the application consistently.   \n \nProblem: Constituent characteristics are not showing for newly launched indices.  \nSolution:  User training issue - no corrective action is needed -   \n \nProblem: Compustat files a re not uploading to EDX Platform due to couple of job failures.   \nSolution:  The access keys have been updated to the latest ones.   \n \nProblem: Automated health checks were failing for https://www.ratefilings.com  \nSolution:  We have updated the configuratio n so that the service always picks up the latest installed \nversion on the machine.   \n \nProblem: EDX API service was not working due to an issue with the resolution of VIP \n(https://soaapi.ws.spglobal.com/edx_prod_spg/).   \nSolution:  \" SOA team suspected ther e was an issue with SOA VIP. SOA team had rolled back the new \ncertificate (SSL), suspecting it to be the cause. This certificate was installed at 06:00 AM (approx.), \nissue persisted.   Network security had confirmed that the external client facing SOA load  balancer \nwas fine, and the inbound connections were being forwarded to the SOA backend servers. EDX team \nhad confirmed that the EDX backend servers were good. SOA team has failed over the traffic from \nSecaucus to Ashburn to mitigate the issue. \"  \n \nProblem : Delay in Xpressfeed, RatingsXpress V3 and V4 files.  \nSolution:  \" INC3086614 - Network team identified that there was a network blip for 6 minutes \n(09:24 to 09:30) ET. Jobs were restarted, and all Ratings jobs are available as of 11:03 ET.   \nINC3161299 -PSS Team restarted these jobs to fix the issue at 11:54 ET. \"  \n ", "doc_id": "e2f9288d-599c-4971-9602-b98426ce1ded", "embedding": null, "doc_hash": "c0672c2a6e46d4922604ff78d5c9fd985afd99b950735cbdadef47eba7a1f200", "extra_info": {"page_label": "13", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 2108, "_node_type": "1"}, "relationships": {"1": "db5fb2f4-d2f0-4d7a-bd62-c66248922ff3"}}, "__type__": "1"}, "661f0b51-3fbc-4dda-b4ff-faaf41c4c843": {"__data__": {"text": "Problem: email notification of CMT RAMP points to RPM but loads nothing  \nSolution:  Now right endpoint URLs been aligned to Simplify application   \n \nProblem: Bug: 4688750: Customer cannot creat e an ARP request due to Enterprise Policy Role \nMissing  \nSolution:  \" There were two fixes associated to this issue. 1) Additional daily synchronization was \ninstituted by the OPDM Team - Synchronization will run between and Opdm (9 am and 5 pm) to \nallow the  system to update changes that come in after 9 am so that the customer is not left waiting \na full 24 hours for synchronizations to complete.   Secondary change was on the Sailpoint side to \naddress the following issue - ARP ACREP Default policy role is not t riggering. This was fixed on 2/25 \nthrough change control. \"  \n \nProblem: ADS not finalized (FR & EDR)  \nSolution:  Fix implemented: Rule base mismatch with the service request - We updated existing \nservice to send the correct rulebase in the rulesets, thus enabling Simplify to consume this data and \ntrigger the closure of the Y case. Code fix deployed to prod on 18th March 2022.   \n \nProblem: Rating Letter Job Upload Issue - RLs are replaced by another RL from a different deal  \nSolution:  corrected the code to stop tagging incorrect jobs   \n \nProblem: Security IDs expected to appear on Ratings360 CLO Collateral Uni verse  \nSolution:  This particular issue is due to the timing and this has been taken care as part of \nContainerization (CHO Data Center Migration) of Data Pipeline solutions.   \n \nProblem: Few of the Capitaliq.com platform functionalities are impacted  \nSoluti on:  Subsequent patches to clustered instances were done manually.   \n \nProblem: Additional rating in Pvt rating_RA -284704 mismatch in history  \nSolution:  Revised the sort logic for Regulation L of the U.S. PCR (Historical Performance of Credit \nRating) for P rivate Ratings so the rating history table is sorted properly.  This was released on 14th \nJanuary 2022.  In addition, RA -284704 where the issue was first identified as been corrected by Ops.  \n \nProblem: User was seeing an error message while trying to acces s GRE. Clearing browser cache \nresolved the issue.  \nSolution:  User cleared the browser cache which resolved the issue and the application started \nfunctioning as expected.   ", "doc_id": "661f0b51-3fbc-4dda-b4ff-faaf41c4c843", "embedding": null, "doc_hash": "43a76a81814dda390ea006a701e9f5be58b28c8368c3fc94bafdbee98b473fa3", "extra_info": {"page_label": "14", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 2304, "_node_type": "1"}, "relationships": {"1": "f1f6da0b-1deb-4625-9d20-4f6720269576"}}, "__type__": "1"}, "2dec25b1-d406-410a-991f-acc3a173e05d": {"__data__": {"text": " \nProblem: There is a FTP connection issue due to which the data did not flow down f rom CS to \nFinmaster.  \nSolution:  \" Root Cause : Upstream source Creditscope rebuild has happened with an emergency re -\nstart required from OpsTeam and was down. Please see the attached communication on the same. \nAs the Creditscope was down, this has caused the existing jobs failures and caused the delay in \nreaching updates to CST system.   Fix : We have requested Creditscope team not to rebuild or apply \nany patches with re -start on business days to avoid these failures going forward. Also including \nconsumers  for the communication incase of rebuild happening in urgency and the latency i.e is \nexpected. This will be fully resolved once Creditscope is fully retired.   Additionally we will review the \nnotification dls to be used for such maintenance notifications s o if there is a need to let additional \nconsumers know of such unplanned activities, we will include those notification dls in the process. \"  \n \nProblem: Hogging thread / High CPU load  average in Prod App servers (Stability)  \nSolution:  As a part of stabili ty initiative, RDR, Docshare Content Servers were moved to new \ninfrastructure and migrated from VM to EC2 instances. Additionally the COSI Client was upgraded \nupgraded to 6.4 version. Related change from 10/14 release is attached.   \n \nProblem: CRISIS : AAR2 -overnight process to calculate and refresh analyst rotation data failed to \ncomplete successfully  \nSolution:  Emergency Change (CHG0218666) was executed on January 5, 2021 to address the field \nsize of ROTATION_CYCLE field in the AAR2.ROTATION table. It was  increased from 1 BYTE to 2 BYTES \nto accommodate double digit rotations (greater than 9). This change was identified as a workaround \npending additional review of upper limits and determination if there is a need to accommodate any \nrotation cycles above 99.  If the max limit of 99 is sufficient after analysis, then this change will be \nconsidered a permanent fix.    \n \nProblem: Zscaler service status is throwing error and i'm unable to connect to VPN.  \nSolution:  The Zscaler Client Connector (ZCC) agent version was upgraded from 3.5.0.108 to 3.6.1.25 \nbased on the recommendation from Zscaler to improve the performance of Zscaler Internet Proxy \n(ZIA) and Zscaler Private Access (ZPA) services.   \n \nProblem: Hybrids : Issues with caching process , which went down making application unusable   \nSolution:  Wipro changed the permissions   \n \nProblem:  Unknown Macro Errors while accessing some of the Platts Wiki Pages  \nSolution:  In order to resolve the issu e, the wiki site was taken offline and each Prod node was \nstarted one -by-one in order to re -sync settings between all nodes   ", "doc_id": "2dec25b1-d406-410a-991f-acc3a173e05d", "embedding": null, "doc_hash": "36fe76ac2446bba50e3239f09c810ceb8dde8129362fbd971c57210adcb9b24b", "extra_info": {"page_label": "15", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 2748, "_node_type": "1"}, "relationships": {"1": "0ac82c0f-cd7c-47e0-9ff0-aa98f437a149"}}, "__type__": "1"}, "d9e9451f-5a46-4831-91a4-80fef3cc9870": {"__data__": {"text": " \nProblem: IDS and DOT CMS are down and showing 502 error.  \nSolution:  \"Immediate resolution: TOMCAT restart was done to resolve th e issue during the incident \nperiod. Permanent resolution:  1.    CHG0168632 (05/14/2021) - Update target groups PROD -IDS-\nWEB -TG-04 and DR -IDS-WEB -TG-04. Change Deregistration delay from 60 Secs to 600 secs for target \ngroups PROD -IDS-WEB -TG-04 and DR -IDS-WEB-TG-04. Deregistration delay will give enough time to \ncomplete such requests and will help reduce 502 errors. 2. CHG0169205 (05/21/2021) - Urgent IDS \nProduction Release V_21_2_3 to resolve to resolve 5xx errors ADO: 3831551  \nttps://spglobal.visualstudio.c om/SPDJI/_workitems/edit/3831551 1. To avoid dotCMS container \nrecycling when response time becomes high  2. To complete the ECS migration of the Apache (Part 1 \nimplemented in CHG0130317) Below are the changes implemented. 1)  Increase DB connections  \nfrom 1 50 to 300 2)  Change JVM memory parameters (move from initial percentage to XMX \noption) 3)  Tomcat Catalina output logging to console 4) Apply Oracle recommendations to \nCMSP DB.  5)  Apache and Apache maintenance logging to console 6)  Enable AWS ES Snapshot \nand Restore for IDS  \n \nProblem: News, Ratings Action, Credit Research and Credit Analytic Alerts are down  \nSolution:  Successfully , bring back the servers by restarting the brokers.   \n \nProblem: Autosys: Weekly symbol job (p_iftp_fc_symbol) got failed: Delay  in Posting the weekly \nsymbol files to the business partners  \nSolution:  ETL weekly symbol job ran successfully after the IFTP secondary node was shut back down, \nand read/write permissions were corrected on the archive directory.   \n \nProblem: Ratings Major Incident - Can't save/check in documents  \nSolution:  Session load balancer issue is fixed and deployed in to production. Issue is with \nconfigurations and it has been worked with Open text and Cosi vendor to address the issue  \n \nProblem: Multiple nodes belon gs to subnet 10.25.228.0 & 10.25.230.0 were inaccessible.  \nSolution:  Network Security team extended the bandwidth license to 5Gbs per sec on F5 load \nbalancer virtual device (VA06BigIPSTRInt1/2.mhf.mhc).    \n \nProblem: Users were facing intermittent connecti vity issues on Singapore VPN for internet based \nbusiness applications.  \nSolution:  \"Network Security team confirmed that they don&#39;t have product license contract, \nwhich is expired. Moved SG VPN users to US VPN. Informed to Engineering Team to migrate S G VPN \non ZScaler.\"  \n ", "doc_id": "d9e9451f-5a46-4831-91a4-80fef3cc9870", "embedding": null, "doc_hash": "2e020784239db152094a4530bb418d243d243fcddee796a6dc89521fd5100a4c", "extra_info": {"page_label": "16", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 2511, "_node_type": "1"}, "relationships": {"1": "8e6f8f16-8a3a-4bf5-8b04-0527eb29786c"}}, "__type__": "1"}, "33713b07-43ac-4eda-813a-dfc0f1895573": {"__data__": {"text": "Problem: Arrow global application is not accessible   \nSolution:  Monit script startup was pointing to wrong script name ,hence the managed server failed \nto come up automatically.  \n \nProblem: Latest Research Documents by contributors wer e not being published to the platform \n(Capital IQ Desktop & SPG Platform)  \nSolution:  \"SSIS Jobs were not able to pick up configuration from Config Files. there was no Change \ndone in CIQ release, but after CIQ release, since SSIS jobs were not able to pick  up data from Config \nfiles, we requested GDPS to set the config values explicitly and that fixed the issue. It took some time \nfor backlog to get cleared\"  \n \nProblem: Ratings Major Incident: cannot open GRE filter  \nSolution:  Veda from Middleware team has provided details on the infrastructure issue. From the \napplication site we have migrated to CAAS infrastructure from EC2 so this issue will not happen \nagain.   \n \nProblem: Hi Team, please can you add the the region (Europe) to the pw job on the back end   \nSolution:  Fixed an issue where PW -jobs were not appearing in the Business Liason workbasket due \nto the user not selecting the &#39;Region Providing Rating&#39;  field prior to saving the job. Users \nwill now be required to select the &#39;Region Providing Rating&#39; field prior to saving (or \nsubmitting) the job.  \n \nProblem: IDS 502 gateway error  \nSolution:  \"Investigation complete - Fixes implemented for 502 error s Immediate resolution: \nTOMCAT restart was done to resolve the issue during the incident period. Permanent resolution: 1. \nCHG0168632 (05/14/2021) - Update target groups PROD -IDS-WEB -TG-04 and DR -IDS-WEB -TG-04. \nChange Deregistration delay from 60 Secs to 60 0 secs for target groups PROD -IDS-WEB -TG-04 and \nDR-IDS-WEB -TG-04. Deregistration delay will give enough time to complete such requests and will \nhelp reduce 502 errors. 2. CHG0169205 (05/21/2021) - Urgent IDS Production Release V_21_2_3 to \nresolve to resolv e 5xx errors ADO: 3831551 \nhttps://spglobal.visualstudio.com/SPDJI/_workitems/edit/3831551 1. To avoid dotCMS container \nrecycling when response time becomes high 2. To complete the ECS migration of the Apache (Part 1 \nimplemented in CHG0130317) Below are the  changes implemented. 1) Increase DB connections \nfrom 150 to 300 2) Change JVM memory parameters (move from initial percentage to XMX option) \n3) Tomcat Catalina output logging to console 4) Apply Oracle recommendations to CMSP DB. 5) \nApache and Apache main tenance logging to console 6) Enable AWS ES Snapshot and Restore for IDS \nECS Go -Live Part 2 1. Update unhealthy threshold from 5 to 2 and Timeout from 10 seconds to 5 \nseconds for the following Target Groups. Update health  check path to /health -checks/inde x.html \nPROD -IDS-WEB -TG-04 DR -IDS-WEB -TG-04  2. Update the DNS - spindices.com to point to ECS \ncloudfront.spindices.com -&gt; PROD -IDS-APACHE -PUB -ALB-01-172732379.us -east -\n1.elb.amazonaws.com (IDS -PROD -PRIMARY WEIGHT 255) cloudfront.spindices.com -&gt; DR -IDS-", "doc_id": "33713b07-43ac-4eda-813a-dfc0f1895573", "embedding": null, "doc_hash": "00f60defe700578bd653c9acb4ca23d4f6ca834fa06e1454877ac12b307ded08", "extra_info": {"page_label": "17", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 3024, "_node_type": "1"}, "relationships": {"1": "753a4375-d945-496b-b0f9-751cb0c44b5a"}}, "__type__": "1"}, "5e1bcafc-a3e4-4f71-b71d-60023a8eb4e9": {"__data__": {"text": "APACHE -PUB -ALB-01-1165569200.us -west -2.elb.amazonaws.com (IDS -DR WEIGHT 0) \nlb.spindices.com -&gt; PROD -IDS-APACHE -PUB -ALB-01-172732379.us -east -1.elb.amazonaws.com \n(IDS-CP-PROD -PRIMARY WEIGHT 255) lb.spindices.com -&gt; DR -IDS-APACHE -PUB -ALB-01-\n1165569200 .us-west -2.elb.amazonaws.com (IDS -CP-DR WEIGHT 0)  3. Make sure DR -IDS-APACHE -\nPUB -ALB-01 has proper WAF(You can compare it to DR1 -IDS-APACHE -ALB). Update it if required.\"   \n \nProblem: Heards were not updating in PMC, Platts Platform and Eclipse  \nSolution:  PAS and Dev team worked together to clear the filtered content queue on PRP Sonic \nserver &amp; restarted the Sonic services, Marklogic DBOps team manually triggered the messages \nwhich were not triggered.   \n \nProblem: change DG title and ensure DG is also ad ded to another section --needed on rpm, wfm and \nsimpifly  \nSolution:  To comply with regulatory requirements and to align with the publishing of the Social \nHousing Providers (SHP) criteria, Disclosure Group &#34;USPF: Revenue &amp; Enterprise \nDebt&#34; was r enamed as &#34;USPF/ IPF: Enterprise and Revenue Debt&#34;. The revised name \nwill appear in the Sector -based Disclosure Group dropdown of RPM, in between the SovIPF and USPF \ndisclosure groups. When this disclosure group is selected for either USPF or Sov/I PF, the existing \ndisclosure text will be displayed in the PCR.  \n \nProblem: Users were unable to access older files in Shared Drive \" \\\\ny01fil500 \\\"  \nSolution:  Wipro Storage team had rehydrated the archives files back to the primary storage to avoid \ncausing recurrence issues and decommission the EOL archive owned and managed by Vendor \nEMC/CDC.  \n \nProblem: R360 : Sovereign : Financial Sector tab is NOT loading  data due to race condition issue  \nbetween DSMP and RDSH generating empty payload   \nSolution:  The rac e condition in spscores_ref.proc_scores_ack that DSMP event was committed \nwhile the ack status hadn?t. The commit is at the very end while DSMP insertion is before the \ncommit. This can be easily fixed by calling the \nDSMP.PKG_EVENT_QUEUE.Insrt_event_ext_tra ns_control instead of the \nDSMP.PKG_EVENT_QUEUE.INSRT_EVENT_INFO. Give the commit control to the caller.   \n \nProblem: Users were facing EDX slowness issues for QTS and Ashburn.  \nSolution:  Issue was fixed by EDX Failover to Ashburn and then increasing TCP Ti me OUT on ftps -\nedxprdqts.capiqinc.com from 5 min to 60 mins   \n \nProblem: SNL.com website was intermittently failing to resolve from multiple locations globally.  ", "doc_id": "5e1bcafc-a3e4-4f71-b71d-60023a8eb4e9", "embedding": null, "doc_hash": "a846ec3903104e40f6563f33979de5ec9ffc8e0a27cc9a28fd59a501f144f417", "extra_info": {"page_label": "18", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 2533, "_node_type": "1"}, "relationships": {"1": "1dc95323-f764-49e5-8bc9-7e1f7a8c526c"}}, "__type__": "1"}, "b038d056-7606-4f71-b397-599027cd66a4": {"__data__": {"text": "Solution:  Ting Vendor informed they are working with their existing vendors to improve DDoS \nmitigation services to address and rectify DDoS attacks in a timely manner also revise their \narchitecture to further improve the overall security against malicious traffic.  \n \nProblem: Simplify System is not working in Production as we expected for Backup Tea m Leads when \nreassigning cases  \nSolution:  \" Issue has been fixed and now all the back up team lead can edit manage assignment and \nsave data as per need.   Before code fix , there was a glitch on existing code where Application is able \nto find only 1 Back up team member and that to be wh o is the 1st row member and discarding all \nother.??     After the code fix,   now Application can able to identify all the back team Availability \nautomatically and each Back up team Lead? member can able to edit the Presenting analyst or \nsupport analyst me mbers. \"  \n \nProblem: Users were facing intermittent connectivity  and slowness on Serv -U (ftp.capitaliq.com and \nftp.Clarifi.com).  \nSolution:  As per the vendor recommendations team have increased of CPU and Memory from 2 CPU \n4GB to 4 CPU 8 GB through CHG016 5722 to resolve the issue.  \n \nProblem: T3 used old weight rules in the system (with effective date of 20200922) for S&P SMIT 40  \nSolution:  We had a check in the system, system will fails the calculation on proforma date if index is \nrebalancing and weights are not uploaded as rebalance date for all the stocks which are part of the \nindex, but this is broken in one of the old release, we identified it recently and fix is implemented as \npart of SPDJI \\Release 21.07 \\Sprint 21.07.02  \n \nProblem: Ratings jobs are del ayed on RatingsXpress V3  \nSolution:  Network got glitched some how and jobs got failed, we have restarted them accordingly .  \n \nProblem: Portal logins are failing for Production CIQ Platform  \nSolution:  CIQ Support team started the services manually on thos e servers post which teams \nconfirmed that the Portal login issue was fixed at 5:58 AM ET  \n \nProblem: Article Service failing intermittently due to out of memory error  \nSolution:  Team confirmed all problem tasks were completed and work was done to expand Fa aS \ncapacity to autoscale based on volume of data.  \n \nProblem: Users from different locations were unable to connect to AWS Workspaces  ", "doc_id": "b038d056-7606-4f71-b397-599027cd66a4", "embedding": null, "doc_hash": "7ef91d5b10986a4631ce434a974da186346ffd9fe47faa3f19b24c6eb82ec99d", "extra_info": {"page_label": "19", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 2354, "_node_type": "1"}, "relationships": {"1": "b47d24b0-5040-4c7c-8b6e-2beeb5d904d7"}}, "__type__": "1"}, "71ad4c76-4d00-47bc-8117-97ecd00de777": {"__data__": {"text": "Solution:  Vendor AWS Support disabled the original update script for all Prod AWS. Teams then \nrenamed the PCoIP agent upd ate script to bypass the script execution and observed that the \nmachines did not reboot automatically. The &#34;Rename script&#34; Powershell script then was \nexecuted to all the impacted systems, post this execution of script users had confirmed that they \nwere able to login.   \n \nProblem: Issues with Hydra Pages Service on Singapore Cloud S&P Global Platform. (Mitigated)  \nSolution:  \"Detect:  We got Datadog alerts  for P95 for stack 2 Singapore for Hydra data service and \nalso for High number of Requests being  stuck on AS2SOAHDTP162 server. Response - Singapore \nstack 2 was pulled out  of the mix  to avoid any issues to the customers  and the server \nAS2SOAHDTP162 was rebooted to resolve issues. Prevention - Process to automatically pull the \nserver from the mix if  the number of Requests on the server starts piling up \"   \n \nProblem: Intermittent slowness issues with the Sourcing Tab in Celsus Application.  \nSolution:  We moved few apps [AM,OIDMapping,MSH Uploader &amp; CRC generation] to point \nQTCOLAPPD(05 -06) servers  instead of QTCOLAPPD(01 -04). It got reduced the load for celsus users \nand performance got increased.  \n \nProblem: Not receiving Emails | Nutley Environment  \nSolution:  When the content switch was reloaded functionality was restored.  \n \nProblem: GCC users were unable to create and publish news to MI platform  \nSolution:  Rebuilt indexes for articleindex table and added the table to automated stats updates \nprocess  \n \nProblem: High CPU utilization in IISFGR which result in DB performance issu e - 4/7/2021  \nSolution:  We are good after moving to new VM  \n \nProblem: Logout functionality was intermittently failing on CapitalIQ Platform.  \nSolution:  \"This can be closed a permanent fix. As team took necessary steps as below -  ? to \nensure any routing changes should not be performed during the weekdays and should go through \nCAB/TAB approvals( PTASK0017756).  Updated SOP for Emergency change (PTASK0017755)  \n \nProblem: Intermittent slowness on ServU (ftp.clarifi.com & ftp.capitaliq.com) .  ", "doc_id": "71ad4c76-4d00-47bc-8117-97ecd00de777", "embedding": null, "doc_hash": "167dfbc12ed901bf9d3cd167b34414462cdbf3b123e57c3d229a9c3e2cda7be9", "extra_info": {"page_label": "20", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 2173, "_node_type": "1"}, "relationships": {"1": "a056aca2-ffcc-4d8b-a416-ddfe4e048b1e"}}, "__type__": "1"}, "b5017d41-ed6a-471d-a332-a30527a59782": {"__data__": {"text": "Solution:  We id entified resources crunch on servers.  We requested Wintel team to upgrade the \nRAM from 4GB to 8 GB, it is an VMware Wintel team upgraded the RAM to 8 GB on file. Contention \nissue was resolved.   \n \nProblem: [CRISIS]  Multiple Iframe dependent applicaitons a re not opening in chrome when \nlaunched through Ratings Gateway.  \nSolution:  The problem is fixed by following steps: 1. Installation of webgate binaries which as \nsuggested by Oracle ? IDM team has the binaries (and add a WG UDF parameter) 2. Added \nSecure;S ameSite&#61;none lines in apache extra.conf file. 3. Cookie domain fix for Pega   \n \nProblem: IDS site is down  \nSolution:  \"Due to the Large import done by Web team , the IDS site was inaccessible for 3 mins. \nAfter the Rolling restart of IDS Tomcat servers ,  the IDS nodes was healthy. Users have been advised \nto perform bulk uploads  at low traffic times and to space them out such that you can confirm the \nfirst one is complete and synced across nodes before starting the next one.\"  \n \nProblem: Docker images in A rtifactory  preventing containers from starting  \nSolution:  Team restored the images to artifactory and amended the scripts that were used as part of \nthe cleanup process.  \n \nProblem: Users not able to Submit documents through DirectConnect  \nSolution:  Serve r side confi was updated and deployed to prod.  \n \nProblem: Mismatch ratings on R360  \nSolution:  Update the cache duration to zero seconds from 2 hours so that no delays on the ratings.  \n \nProblem: Slowness and errors with CIQ Excel plugin and CIQ Web Portal Login.  \nSolution:  API Gateways have the connection count thread changed from 128 to 1000 and restarted \nthe services   \n \nProblem: Industrial and Commercial Bank of China (New Zealand) Limited Issuance Ratings Missing  \nSolution:  \"Updated the hydrob to fix the issue and verified the fix on lower environments. It will be \ndeployed to production as per the schedule.\"  \n ", "doc_id": "b5017d41-ed6a-471d-a332-a30527a59782", "embedding": null, "doc_hash": "9d5b682c62f555912c596c214c47eab6ba7d5b28e64afe3a7979f05c01a679a6", "extra_info": {"page_label": "21", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 1958, "_node_type": "1"}, "relationships": {"1": "e6923e67-679c-4881-af4d-3a50ba1f1fd1"}}, "__type__": "1"}, "5ad07fdc-a14b-47ef-b211-e2e12d39411c": {"__data__": {"text": "Problem: CIQ portal and Excel Plugin IDM 12c delays due to the CIQ 12c Webgate API gateways \nservicing the Plugin calls exceeded limits  \nSolution:  On the API Gateway servers we increased the CPU and Memory resource by 2x and also \nincreased the concurrent connection  pool configuration by 4x to allow for more concurrency and \nreducing the thread saturation.?   \n \nProblem: CreditPro experienced an outage.  \nSolution:  The issue was resolved. Tanium scheduling was put in place.   \n \nProblem: Users are unable to see APHub Sear ch screen ( While login)  \nSolution:  SearchEmployee / GetEmployeeDetailsBy ID  service is returning huge response  for \nGeneral comments as result user is unable to see the complete ApHub screen due to response time \nout. we have suppressed General comments in response while user logins into APHUB application  \n \nProblem: Price metric was failing on charting and corporate profile on S&P Global Platform.  \nSolution:  \"the issue was caused as an unexpected update was pushed onto the proc. the issue was \nresolved wh en the 4 procs where updated to the expected status by jason johnson from the RSA \nteam .\"  \n \nProblem: Pipeline that brings Commodity Data from CIQ Platform  to MI Platform is down.   \nSolution:  Issue was resolved by re -processing the data and no more incide nts of this type have been \nreported as of the end of 2021  \n \nProblem: Can't access  https://neo.app.tricentis.com/ and https://vision -ai.app.tricentis.com/   in IE \nand Tosca Vision AI Agent  \nSolution:  This issue was resolved when a package for the software  installation was created and \npublished to software center.  \n \nProblem: CAP QA Loading issue  \nSolution:  query fine tuning in QA and UAT DB fixed the issue  \n \nProblem: Informatica Services were impacted in Charlottesville DC  \nSolution:  Network security team  has applied the fix by reloading the network firewalls and double \nthe IP pool capacity at 23:50 01/26/2021.? Initial test results look positive and application team \nconfirmed that there no more errors reported from informatica services.   ", "doc_id": "5ad07fdc-a14b-47ef-b211-e2e12d39411c", "embedding": null, "doc_hash": "4fe14189dd63123a94c0f8981b1ae22323444ae5b2e237ffe310077bccd9cf23", "extra_info": {"page_label": "22", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 2083, "_node_type": "1"}, "relationships": {"1": "0f3bb16b-320f-47b1-be61-26b98e52f6e3"}}, "__type__": "1"}, "4ec7c74c-bb46-41b4-a705-4e2d5783ec7b": {"__data__": {"text": " \nProblem: SQL Se rvice was restarted on DMZSQL3  \nSolution:  \"DMZSQL3 was restarted due to heavy purge activity on OnlineLogging.SessionState table \ncausing high threadpool count/active sessions/blockings. Further analysis showed that the said table \nhad high index fragmentatiom. GDPS have performed index m aintanence of that table on 30th \nJan&#39;21 through the CHG0150921. Post the index maintenance, we do not see any issues with \nDMZSQL3.  \"  \n \nProblem: Parc Transaction Log is full due to replication  \nSolution:  Replication was not working for TelekursData an d the log reader agent had filled the \ntransaction log up filling up the drive. I fixed replication and shrank the log. I also created a 2nd log \nfor PARC to prevent this from happening again.   \n \nProblem: MHF1 users are intermittently unable to access folder s/ contents within shared drives  \nSolution:  Reported issues were resolved by reapplying / re -adding permissions &amp; domain. Not \nreproducible anymore.  \n \nProblem: Research Articles for Realtime ratings, RD on CIQ and Xpressfeed files were delayed. The \nimpacted files from Xpressfeed are Rating V2,V3,V4 and Credit research   \nSolution:  The issue started on QTDFPDBPD01 Server got crashed due to that ATHENS with golden \ngate services failed over to  QTDFPDBPD02 from QTDFPDBPD01 but services not started \nautomati cally. Manually started golden gate services on Passive node QTDFPDBPD02 fixed the issues  \n \nProblem:  FIM connector showing empty values on Simple Access account creation for non \nemployees.  \nSolution:  \"Permanent fix is deployed by: - - Removing the offendi ng DC (NJ12ADC001) on 23rd Dec \nand updating the healthy ones - Team has monitored this for three months and no issues found. \nThere were accounts in queue that did not get auto processed, hence few issues were noticed in \nJanuary. - Exports are running fine via NJ12ADC003, Auto provisioning of accounts is in place now.\"  \n \nProblem: PLT ECLIPSE XPLORE: Latest outage data was not populated for NCS due to one tableau \nextract getting timedout  \nSolution:  To mitigate the issue DBOPs Team changed the execution to li ve connection of a single \ndata source to get the Realtime data.   \n \nProblem: Users working from Manila & Pune ODC (SPGI & IHS) were unable to access internet via \nLAN due to a power failure on a legacy switch in Secaucus DC  ", "doc_id": "4ec7c74c-bb46-41b4-a705-4e2d5783ec7b", "embedding": null, "doc_hash": "7469f1917949f05a6e6abb2f69f975ade2200d8a55f44854bfed2905e2f94261", "extra_info": {"page_label": "23", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 2364, "_node_type": "1"}, "relationships": {"1": "38477e1a-89a1-4db3-8d3b-140fd93614f7"}}, "__type__": "1"}, "a96dc815-3bd6-4b57-8981-f62394a4f93e": {"__data__": {"text": "Solution:  The onsite Engineer vi sited the Datacenter and powered on the switch which mitigated \nthe issue.   \n \nProblem: IndexMetadataFeed - Error occurred when validating remote object \nGRIP.INDEX_DEST_FOR_IDS@GRIPXP  \nSolution:  DV team have executed the ARPING command for the Nutley DNS   \n \nProblem: Meeting tab is showing as not entered for publishing job  \nSolution:  Permanent fix applied? by removing hard coded values in meeting list and passing event to \nthe function to call process action.   \n \nProblem: Case Shiller data Files not distribute d to EDX causing delays of data updates on IDS  \nSolution:  IMPG team corrected the file names (added hash tag around the date part) and did an \nimmediate posting of the data files from CARE to EDX only.   \n \nProblem: Missing MD from vendor  \nSolution:  This was fixed during Fierce release 6.31.1 5818907 PRB0062750 - Missing MD from \nvendor   \n \nProblem: Hyperlinks are not working from Outlook in Citrix VDI.  \nSolution:  \" Citrix vendor advised a registry workaround  to add Outlook.exe to exclusion list of Citrix \nAPI Hooks. So that the Citrix API involvement of calling Outlook.exe processes will be excluded and \noutlook.exe runs on its own processes to execute. Tested the workaround by getting patching team \ndeployed th e Microsoft patches on the VDIs with registry added.   \nHKEY_LOCAL_MACHINE \\SYSTEM \\CurrentControlSet \\Services \\CtxUvi Value Name: \nUviProcessExcludesType: REG_SZValue: outlook.exe   Citrix Team has deployed the registry to all the \nimpacted VDI based on the use r reports which resolved the issue. \"  \n \nProblem: Search for Object cannot be performed when there are special characters in Entity.  \nSolution:  \" When calling Search object service pega will be passing escape character for the service \nto be able to parse w hen there is a double quote character in the search string   CR:? CHG0323638    \nRWI:?  https://spglobal.visualstudio.com/Ratings/_workitems/edit/5906140   \"   \n \nProblem: Models / RAMP Template  / Documents opening in RAP eventhough it's not integrated.  ", "doc_id": "a96dc815-3bd6-4b57-8981-f62394a4f93e", "embedding": null, "doc_hash": "3985f12b776f9610c818856a16414b741614bda6fb0d8704571adc88ad776c8d", "extra_info": {"page_label": "24", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 2070, "_node_type": "1"}, "relationships": {"1": "44cf608d-4bdd-4f4f-b0a7-bc766fa148bd"}}, "__type__": "1"}, "5d615a6d-f425-493f-9b82-1815ff0a223b": {"__data__": {"text": "Solution:  We&#39;ve added auto &#39;refresh&#39; activity to the placeholders so that upon each \ndeletion of a document/placeholder, the mapping is refreshed and that the new placeholder is \nmapped properly, it overrides the previous mapping details in Simplif y DB.   \n \nProblem: Errors while saving in CTS institution form and delay in delivery of latest Company Profile \ndata to CIQ Pro platform due to Middle Tier release.  \nSolution:  \" It was identified that the issue was caused due to &#39;SHIRE API&#39; service was \nreturning failed requests. &#39;SHIRE API&#39; service started failing after new version of \n&#39;MTS Nuget&#39; was released MiddleTier Release on 12/6 (Middle Tier - Sprint 23.01.03 - \nCHG0297109), to support future functionality for non -transactional batches.   MI DataServices \nBteam stated that this functionality should not have had consumers in production. However, \nbecause SHIRE was incorrectly using non -transactional batches, therefore application started \nattempting to use this incomplete functionali ty. MI DataServices B team upgraded the MTS NuGet to \nsupport non -Transactional requests and deployed it in lower environment for testing. Validations \nwere unsuccessful in staging and it was identified that queries are even failing with the latest version \nof MTS NuGet in staging and expected to fail in prodcution as well, due to Postgres version mismatch \nbetween lower(11.16) and prodution (10.2) environments, as older Postgre version does not support \nexception handling.   MI DataServices B team then made the  code changes to MTS Nuget to force \nthe SHIRE calls to use only &#39;Transactional&#39; batches to correct the behavior from the \nbackend. ReleaseEngineering team deployed the fix in production afer successful validation in lower \nenvironment, to mitigate th e impact at 10:59 AM ET.(ECR# CHG0298685).   Company Profile data is \nflowing as expected as of 10:59 AM ET and the issue is resolved. \"  \n \nProblem: Errors in RIL Workflow  \nSolution:  \" Index key profile got changed from 04/01/2019. When validating the profi le with the \nsource, the code is considering the latest index key instead of the current day index key. DML was \nexecuted in PROD &amp; Staging to address this scenario and new transaction was triggered in \nStaging.  \n \nProblem: unable to ping or remote into an y production machines in EWDC  \nSolution:  Team to upgrade the software of the switches on Development and the Production \nenvironments.  \n \nProblem: Impact Assessment: SFDC email to case (E2C) delays --SPDJI  \nSolution:  Salesforce?s 3rd Party vendor made a sof tware update that caused emails to bounce. \nOnce the update was rolled back, emails resumed sending   \n \nProblem: Unable to create new files on Drive D on DMZPROCESS4 due to  Volume D had ReadOnly \nattribute set to \"Yes\".  ", "doc_id": "5d615a6d-f425-493f-9b82-1815ff0a223b", "embedding": null, "doc_hash": "155ee1bdaf26d97c11d9ee7d0db956245aeabc0029893fafbba567dbbd5e9085", "extra_info": {"page_label": "25", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 2831, "_node_type": "1"}, "relationships": {"1": "a3d150e6-0daf-4eb8-8ad2-0fe8e9f9e55c"}}, "__type__": "1"}, "1e41b9a9-3f49-4fde-8997-08dd4dc22756": {"__data__": {"text": "Solution:  No issues reported after l ast incident, this was due to backup set drive to read only  \n \nProblem: Criteria Workflow Reviews application is not accessible via Ratings Gateway  \nSolution:  This issue occurred after the change CHG0057015 was implemented for patches deployed \nfor vulnerab ilities and exploits as part of a vulnerable remediation project, post patching one of the \nweb server was not started as criteria workflow application were not aware this application has 2 \napache instance and performed the post validation test only on the non SSO url and had not verified \non the SSO url testing due to which users using the SSO url were facing the error on accessing the \nCriteria Workflow Reviews application  \n \nProblem: Production: CRITICAL: COL10DBSQLPRD failed over to HYCOLDBPD01 from HYCOLDBPD02  \nSolution:  Installed Microsoft Visual Studio Tolls for Applications&#39;  and configured proxy settings \non HYCOLDBPD02. After these fixes, tested them by failing over  the instance to HYCOLDBPD02 and \nconfirmed that the jobs are running fine.   \n \nProblem: No price updates on RDF after receiving a 'Record Dropped' message from Reuters  \nSolution:  Wombat team implemented a code fix for this issue.   \n \nProblem: IR websites an d MI Platform were facing intermittent issues  \nSolution:  \"Root Cause Code: Configuration Error As per the Case # 119021119653087 raised with \nVendor Microsoft, the team had evaluated the cause of spinlock contention on database servers - \nDMZSQL1 &amp; DMZS QL3 as slow TempDB objects creation which was impacting the performance \nof OLP DB. Additional Information: To mitigate the issue, Vendor Microsoft had suggested to enable \nthe Trace log 8005 and also suggested for modifications of stored procedures which we re using \nTempDB heavily. Post which, as a permanent fix, Vendor Microsoft had shared the on -demand hotfix \nto reduce the Spinlock contention.\"   \n \nProblem: SAN storage issue impacted limited functionalities on the CIQ platform  \nSolution:  Implement bug fix o n storage arrays by upgrading version from MU4 to MU6.  \n \nProblem: Latency issues with running certain functionality for  Deal Analyzer  \nSolution:  The underlying issue was the way storage was configured post AWS migration of the \napplication.  There were st ill dependencies on the legacy NAS system. Once the storage subsystem \nwas updated to use AWS technology, performance was vastly improved  \n \nProblem: RCA - INC0799484 - P2 - Connectivity to secure ftp server(sftp.platts.com) was down  ", "doc_id": "1e41b9a9-3f49-4fde-8997-08dd4dc22756", "embedding": null, "doc_hash": "ac922782f5e79ac1646155faad2210789af4ecd70fc6e0d01b5a7cd7af4537bd", "extra_info": {"page_label": "26", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 2534, "_node_type": "1"}, "relationships": {"1": "9aa51383-b563-46bd-b62d-cef1f558353c"}}, "__type__": "1"}, "4b0d2c59-205e-4c05-a65d-df5dcf4eee62": {"__data__": {"text": "Solution:  The outage i s due to CPU utilization is high  \n \nProblem: SSO is down globally for external products, MI platform and capital IQ  \nSolution:  As per Vendor F5 Product development ID 490174, need to update the load balancer OS.  \n \nProblem: XML documents not rendering in MI  Platform  \nSolution:  The style sheet was reverted in our Amazon S3 repository around 10:00 AM. However, any \nuser that opened a document and leveraged the improper style sheet between 4:18 and 10:00 AM \nwould continue to encounter an error until cache was c leared from the UDR buckets. Therefore, the \ncache was manually cleared, and functionality was restored for all XML files around 11:30 AM.   \n \nProblem: Major Incident - Market Intelligence - Conning client from US -Dallas location experiencing \nauthentication errors while accessing S&P Global and CIQ Platforms.  \nSolution:  as part of the Vendor Verizon case # 2020031035758, Verizon added statement (network \n204.148.83.44 0.0.0.3 area 0) in OSPF process on router EQR5 (mchill -eqr5 -20525789e001) which \nwas missed b y Vendor Verizon during their change implementation for the 1 to 10 Gig upgrades.  \n \nProblem: Monitoring Alert reported slow response times on in RatingsDirect on CIQ - INC1498796   \nSolution:  Increased the heap size and RAM on the server   \n \nProblem: Simplify CCST Circular Reference Error (Case ID: C26367)  \nSolution:  Model was updated and since the update there are no longer circular reference errors \npresenting with this model.  \n \nProblem: Major Incident - Market Intelligence - Automated Health checks of SPGP (On -prem): All \nServices/All Users - are failing for random locations  \nSolution:  As per defined process,  asked NOC to restart Spark Corosync services and then guided \nthem to rest art the spark servers which fixed this issue.  \n \nProblem: Not able to log into DotCMS for Vault  \nSolution:  Restarted tomcats in rolling fashion .Issue was resolved.  \n \nProblem: Incomplete Proforma report generation  \nSolution:  Added a report validation job before reports are triggered   ", "doc_id": "4b0d2c59-205e-4c05-a65d-df5dcf4eee62", "embedding": null, "doc_hash": "82e5375390981597062078e909c5e3036a520e7ab5c3054a446fb10d5b464fe2", "extra_info": {"page_label": "27", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 2069, "_node_type": "1"}, "relationships": {"1": "6d86bbde-2abd-45dd-9a14-8f4f34f430e6"}}, "__type__": "1"}, "8bac434a-3bef-4f11-aef2-2b646984b09f": {"__data__": {"text": " \nProblem: WFM Analyst location change from Dallas to Farmers Branch  \nSolution:  Updated with changes to include Farmers Branch Location  \n \nProblem: Major Incident - Market Intelligence - unable to access/login to multiple CIQ production \nservers  \nSolution:  Unix servers were configured qts -dev-dc1.ciqdev.com and this server was rebuilt with new \nOS and new name, nj15dc003.ciq.dev.com. Unix support team replaced the old dc name with the \nnew one to resolve the issues however as a p ermanent fix AD team has deleted the ciqdev.com dns \nstub zone from qts -inc-dc1.capiqinc.com and recreatedit. Support teams confirmed that they were \nable to login to Capiqinc.com Unix machines with Ciqdev ids.   \n \nProblem: Major Incident - Market Intelligenc e - Data is available in the IR Console but it's not visible \non the IRW page   \nSolution:  Fixed index optimization job to prevent future occurrences.   \n \nProblem: RCA - INC1421688,INC1389851 - P2 - PEA (Platts Excel Add -In) & API's - Streaming \nConnections w ere unavailable to internal and external users and hence were unab  \nSolution:  PAS Team has restarted the Kaazing services on node1 and node2 to resolve the issue.   \n \nProblem: RCA - INC1355829 - Roll over dates were missing for London gas oil data for Dece mber 13  \nSolution:  Database team manually inserted the rollover data as a temporary fix.?   \n \nProblem: Duplicate PCR jobs created because of agent running o two nodes  \nSolution:  Coding change made to ensure the PCR Non -Workflow agent only runs on one node.  \n \nProblem: CTS Financial queries troubleshoot  \nSolution:  MT Trigger was changed the way it was working to resolve the issue.  \n \nProblem: Re -post Reg Jurisdiction repo rt  \nSolution:  Workaround was done to manually upload the file to S3 bucket  \n \nProblem: The exisitng National scale rating does not display while cascading the rating in Simplify.  ", "doc_id": "8bac434a-3bef-4f11-aef2-2b646984b09f", "embedding": null, "doc_hash": "64aabf3d8bf24a2b57691e476baabe9f8695b25cf1af6593a64687d46d39d4de", "extra_info": {"page_label": "28", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 1895, "_node_type": "1"}, "relationships": {"1": "0ee29ad9-bc57-4b31-a6dc-9e87108f72f0"}}, "__type__": "1"}, "10c833d9-eda6-4c78-82c8-1153167f9291": {"__data__": {"text": "Solution:  Scripts  are  applied to fix the issue  \n \nProblem: Issues with ESP  and Linx following wan optimization  \nSolution:  To resolve the issue Wipro NW team have bypassed the traffic in the riverbed from the \nsource &#34;Any&#34; to the destination server IP 10.169.40.133, problem is co -related to WAN \nevolution project  \n \nProblem : Facing issue while connecting  password for EQI_FGR_CLIENT on IISP  \nSolution:  fixed it by placing the single quotes around the existing password in connections -AWS -\nPREPROD.properties in preprod environment.  \n \nProblem: CST is not capturing FY2019 'AUDIT'  numbers  \nSolution:  DVL cache needs to be refreshed post the FinMaster data load.  \n \nProblem: The criteri and guidance \"Alternative Investment Funds Methodology\" was published \nyesterday, we are not able to find the guidance in to Model Repository.  \nSolutio n:  criteria search is updated to search for all criteria  \n \nProblem: W -341897 - showing automatically as cancelled  \nSolution:  Enhanced the system to not automatically cancel jobs after 6 months of creation as long as \nusers are actively working on it.  Use rs will now receive multiple email notifications before an idle \njob is cancelled.  \n \nProblem: In Informatica |  Webinfop_Rep spice folder, most workflow are long running  \nSolution:  WFs migrated to Spark  \n \nProblem: Job folders are getting linked to taxonomy  in DocShare  \nSolution:  After enabling all failsafe jobs issue got fixed  \n \nProblem: COMPARE_GDB_EOD  \nSolution:  DML was executed to resolve the issue permanently   \n \nProblem: Template upload is not allowi ng indices that it should be  ", "doc_id": "10c833d9-eda6-4c78-82c8-1153167f9291", "embedding": null, "doc_hash": "de6f69c78e220346a65a49eedaf0dd8bb8260e6ec2975bf819c7186e5909e8e7", "extra_info": {"page_label": "29", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 1633, "_node_type": "1"}, "relationships": {"1": "db1976cc-e5b7-471a-9c7f-2a38bdae3a14"}}, "__type__": "1"}, "8842e3dd-3d9e-436f-ba6f-46cd009d2030": {"__data__": {"text": "Solution:  Issue has been resolved by executing a DML  \n \nProblem: New Report: All -Time High and Low report  \nSolution:  Issue has been resolved by executing a DML -Closing the Ticket  \n \nProblem: Translation issue   \nSolution:  As p er request changed the role wording and fix available from 6th Dec 2019  \n \nProblem: AoD(Analytics on Demand) Portal Not Working  \nSolution:  This issue was due to java vulnerability work that was being done to update the vulnerable \nversions of java in the en vironment.  It was later resolved by restoring java to the server.   \n \nProblem: Major Incident -Market Intelligence - Slowness in loading Credit Research and Investment \nResearch pages on CIQ Platform.  \nSolution:  Added 2 searcher nodes to IR search cluster  \n \nProblem: Server ashrtapiprd12.prod.mktint.global  was not reachable   \nSolution:  Unix team had worked with MI Dashboard team to get the ILO IP and brought up the \nServer - ASHRTAPIPRD12, post which the issue was resolved.  \n \nProblem: Xpertdoc DB Issue  \nSolu tion:  The transaction log has been set to auto resize and alerting is no monitored via two tasks  \n \nProblem: AAR is not working - ERROR  \nSolution:  one of the web server was down we have brought up and fixed the issue.\"  \n \nProblem: Cannot upload Documents to Box   \nSolution:  The Box vendor has implemented the permanent fix to the issue.  \n \nProblem: Major Incident - All Divisions - Ticket Crea tion Not working in Snow.   \nSolution:  Access control restriction implemented  \n ", "doc_id": "8842e3dd-3d9e-436f-ba6f-46cd009d2030", "embedding": null, "doc_hash": "3d68a28440ac921d284180506851dda1820338ee2651aa4c46048edda68c0a5f", "extra_info": {"page_label": "30", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 1505, "_node_type": "1"}, "relationships": {"1": "51abef16-717a-4c3a-b3fd-3c1244771028"}}, "__type__": "1"}, "6ccd872f-ceef-43e5-a9b5-38570afb9b43": {"__data__": {"text": "Problem: AM user's are unable to launch the Look Back Review in production after Pega822 \nupgrade.  \nSolution:  Issue  got resolved by resaving the rule of   &#34;Data -Portal.Sho wDesktop&#34;  to \n&#34;Data -Portal.ShowDesktop_LB&#34;     and now either of them will work and should not be \nproblem anymore.  \n \nProblem: The workflow flow ID is showing as undefined in WFM screen  \nSolution:  Fixed the code to verify that Job ID value sent  all times.  \n \nProblem: EASIDs being excluded when they shouldn't  \nSolution:  Fixed include/Exclude issue. When one of the ASIDs is excluded (having common EASIDs \nwith other ASID included)EASID is touched and move to final ratings and recalcualte, the EASID s at \nthat point are excluded from the job itself.  \n \nProblem: Report Generation not  starting after Index Calculation  \nSolution:  Report Thread Count was increased on 11/22 to fix this issue.  \n \nProblem: SPT - Surveillance & Publishing Tracker - Production  \nSolution:  Added proper NULL checks for full review date objects in DROOLS code.   \n \nProblem: No Stream Display Errors and Slowness issues in Simplify - Post Patching.  \nSolution:  The MDC settings were updated in 12 C for this issue.   \n \nProblem: DSOS North America Universe (Universe_id=442 ) index calculation issue   \nSolution:  Index setup is fixed and issue is resolved. Closing this problem ticket.   \n \nProblem: LinX performance issues from Dallas, Boston, San Francisco, Chicago location  \nSolution:  Resolved the issue by moving configuration to local machine  \n \nProblem: I am unable to access the pending document basket. Every time I click into pending \ndocu ment the system jammed and I cannot click into any job at all.  \nSolution:  Fixed loading/performance issue on the Pending Docs Workbasket for SF and CGS by \nadding pagination, each page of size 200 records.   Users will see improved loading time.  ", "doc_id": "6ccd872f-ceef-43e5-a9b5-38570afb9b43", "embedding": null, "doc_hash": "53d23420100bc2e192be2bca9948b6a8e6f50caf37d7f29609eda85c94d0a2e6", "extra_info": {"page_label": "31", "file_name": "Problems_Solutions.pdf"}, "node_info": {"start": 0, "end": 1890, "_node_type": "1"}, "relationships": {"1": "80fb44e1-1a80-4d00-bdfd-c1e4ecc5e4d2"}}, "__type__": "1"}}}